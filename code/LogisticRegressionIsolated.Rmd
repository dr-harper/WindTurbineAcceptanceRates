---
title: "Supplementary Statistical Analysis"
date: "`r format(Sys.time(), '%d %B %Y')`"
author: Michael Harper, Ben Anderson, Patrick James, AbuBakr Bahaj
output: 
  bookdown::html_document2:
    toc: TRUE
    toc_float: TRUE
    code_download: TRUE
    theme: flatly
bibliography: packages.bib
---

This file documents the full statistical analysis and provides detailed R analysis. The file includes all data and functions required to reproduce the analysis.

To run the script, you will have to install the package `WindAnalysis`. This depends on the packages `caret, corrplot, ggplot2, magrittr, Hmisc, broom, car, plyr, psych, stats, ggthemes, grid, DiagrammeR, rsvg, DiagrammeRsvg, scales, knitr`, and therefore the packages will be installed if they are not on the system. Users should install the `WindAnalysis` [@R-WindAnalysis] package using: 

```{r install-windanalysis, eval = FALSE}
devtools::install_local("WindAnalysis")
```



# Setup

```{r setup, message=FALSE, warning=FALSE}

library(WindAnalysis)
library(tidyverse)
library(magrittr)

# General knitr Options
knitr::opts_chunk$set(tidy = TRUE,
                      fig.align = "center")
```

# Load the turbine data

The data is loaded. Adjustments are made to convert the data into a dichotomous variable. Only large wind turbines were focussed on within the project and therefore smaller projects (less than 1MW) were filtered from the dataset.

```{r}
# Logistic Regression Input Files
df_turbine <- read.csv("TurbineFullInfo.csv")

# Removes "Awaiting Decision" Factor from list
df_turbine <- dplyr::filter(df_turbine, Status.Summary != "Submitted")

# Relevel data to make the Reject 0 and Accept 1 
df_turbine$Status.Summary <- factor(df_turbine$Status.Summary, levels=c("Refused/Abandoned","Approved"))
df_turbine$Planning_Status_Code <- (df_turbine$Status.Summary  %in% "Approved") %>% multiply_by(1)

# Further removal of small turbines from analysis
df_turbine_all <- df_turbine
df_turbine <- filter(df_turbine, Size == "Large")

# Variables used in model
variables <- read.csv("StatisticalVariables.csv")
model_variables <- as.character(variables$Variable)
```

------

# Step 1: Data Checks

Statistical models are fundamentally influenced by the quality of the data used to build them. Failing to understand any limitations within the dataset can result in misspecified statistical models. It is therefore critical that checks are conducted to fully understand any potential issues within input parameters before modelling, and that any assumptions required for the statistical analysis are met. This section therefore presents the preliminary analysis conducted to understand the dataset.

### Summary Statistics

```{r CreateSummaryTable}
turbines_numeric <- df_turbine %>% FiltNumericColumns()
turbines_numeric <- turbines_numeric[,!(names(turbines_numeric) %in% c("RO Banding", "NA", "NA.1", "NA.2", "NA.3", "NA.4", "NA.5", "NA.6", "Y Coordinate", "X Coordinate"))] %>% set_rownames(NULL)# Delete RO banding category
summaryTable <- SummariseDataframe(turbines_numeric)[-1,] %>%
  mutate(Variable = matchNames(rownames(.))) %>%
  select(Variable, everything()) %>%
  .[complete.cases(.),]
```

Summary statistics provide a useful overview of the modelling data, and can provide an indication of issues which may warrant further exploration. Table \@ref(tab:SummaryStatistics) presents the statistics for the numeric variables within the model. It can be seen that the geospatial proximity to features used within the study vary significantly in their range. For example, the maximum distance a wind energy project is from *Urban Regions* is `r max(df_turbine$UrbanRegions) %>% round(., 1)`km while *Heritage Coast* has a range of `r max(df_turbine$HCoast) %>% round(., 1)`km. Such a large difference in range can create difficulty in interpreting results, as a unit change of 1km has a much larger impact on the *Urban Region* parameter than *Heritage Coast*.

While previous studies have transformed the parameters to a standardized scale by using a z-score linear transformation [@VanRensburg20], it was decided that they would be left unaltered within approach: such a transformation provides no direct benefit to the model other than to allow a direct comparison to be made between the influence of parameters. In addition, model parameters create additional complication in the generalisation of the model results as models must transformed to the adjusted z-score scale to allow for comparison [@Harrell2001, p.123].

```{r SummaryStatistics}
knitr::kable(summaryTable, booktabs = TRUE, caption = "Summary statistics of the parameters collected for onshore wind energy within the statistical analysis. These values represent the dataset before missing values were imputed") %>%
  kableExtra::add_footnote("Political data represents the number of seats held within the local council, not the vote share. This explains why the range can vary between 0 and 100")
```

### Missing Values

```{r MissingValues}
# Calculate number of missing values
observation_count <- nrow(df_turbine)
percent_missing <- (1 - (complete.cases(turbines_numeric[names(turbines_numeric) != "Political, SNP Share"]) %>%
                           multiply_by(1) %>% 
                           sum)/nrow(df_turbine)) %>% 
  multiply_by(100) %>% 
  round(1)
```


Logistic regression cannot directly deal with missing values within the dataset, and if an observation lacks any single parameter, the whole record will be removed from the data. This can result in a loss of data and potential impact on the modelling results, as a smaller sample size is used to build the model. It is therefore recommended that missing values are assessed, and where appropriate, imputed to create complete records [@Harrell2001, @Griffith2003].

It can be seen within the summary table that there are few cases of missing data across the total dataset (n =`r observation_count`), with a total of `r percent_missing`% observations being incomplete. It was found that Slope, Elevation and Wind Speed missing data was a result of the rasterisation of the input dataset, with sites near the edge of land (rivers, coastline, lakes etc.) being occasionally lost within the dataset. Further missing data was seen within the political data, which resulted from boundary changes in 1995 which made it not possible to map, and the SNP party only exist within Scotland. 

Two techniques were used to impute missing datasets [@MandelJ2015]. Firstly, mean substitution was used to impute missing slope, elevation and political data. For the SNP data, any missing values which were in England and Scotland were imputed as 0. 

```{r ImputeMissingValues}
# Fill missing values
MissingCols <- c("Slope", "UKElevation", "WindSpeed45", "Con_share", "LD_share", "Lab_share", "Oth_share")

for(i in MissingCols){
  df_turbine[[i]] <-Hmisc::impute(df_turbine[[i]], what = mean)
}

df_turbine$SNP_PC_sha <- Hmisc::impute(df_turbine$SNP_PC_sha, 0) %>% as.numeric()
```

### Influential Outliers

Influential outliers are observations where values deviate from the expected range and produce extremely large residuals [@Hosmer2004]. These can result in incorrect inferences from the statistical model, as the model may overfit to these extreme cases. It is therefore recommended that assessments are made before statistical modelling is conducted to identify potential cases which may need removal. 

Figure \@ref(fig:plotNearestTurbine) a) highlights the distribution of proximity variables, and provides a visual method to identify potential errors. The boxplots highlight that many parameters have outliers, which are largely caused by a narrow interquartile range resulting from the low standard deviations within the dataset. As the outliers are relatively evenly spread throughout the dataset, with few extreme values, they are of limited concern within the modelling.

(ref:plotNearestTurbine) Boxplot for proximity variables which were derived from model parameters. A different censoring distance was applied to Spatial and Spatiotemporal datasets (30km and 80km respectively)

```{r plotNearestTurbine, message=FALSE, warning=FALSE, fig.cap= '(ref:plotNearestTurbine)'}

data_turbine_boxplots <- matchNamesColumns(df_turbine)

## Identify which columns have Distance parameters
colref <- which((substr(names(data_turbine_boxplots),1 ,8) %in% c("Distance")) == TRUE) 
data_turbine_boxplots <- data_turbine_boxplots[, colref] %>% reshape2::melt() %>% mutate(id = "a) Spatial")
data_censored <- dplyr::filter(data_turbine_boxplots, value > 30)
data_turbine_boxplots2 <- matchNamesColumns(df_turbine)

## Identify which columns have Distance parameters
colref <- which((substr(names(data_turbine_boxplots2),1 ,8) %in% c("Nearest ")) == TRUE) 
data_turbine_boxplots2 <- data_turbine_boxplots2[, colref] %>% reshape2::melt() %>% mutate(id = "b) Spatiotemporal")
data_censored <- filter(data_turbine_boxplots2, value > 30)

lines <- data.frame(id = c("b) Spatiotemporal", "a) Spatial"), value = c(80, 30))

ggplot(rbind(data_turbine_boxplots2,data_turbine_boxplots), aes(x = reorder(variable, value, FUN = median), y = value)) +
  geom_boxplot(aes(fill = reorder(variable, value, FUN = median)), outlier.shape = 1, outlier.size = 0.5, outlier.alpha = 0.6) +
  scale_fill_discrete(grDevices::rainbow(20)) +
  scale_y_continuous(limits=c(0, 400)) +
  coord_flip() +
  theme(legend.position="none") +
  labs(x = "", y = "Distance, (km)") +
  geom_hline(data = lines, aes(yintercept = value), linetype = 2, col = "grey10", alpha = 0.5) +
  facet_grid(id~., scales = "free_y", space = "free_y") +
  theme(strip.text.y = element_text(angle = 0, colour = "black", hjust = 0),
        strip.background = element_blank())
```

A second boxplot is shown in Figure \@ref(fig:plotNearestTurbine) b) which highlights the "*Nearest Turbine*" parameter dataset. These are represented separately as their derivation used a different approach to the other geospatial proximity parameters, considering both the *location* and the *year* of the development. This has resulted in a small number of extreme outliers which represent sites which were the first to be developed within a region. Only `r filter(df_turbine, NearestTurbinePlanned>100) %>% nrow` sites were further than 100km from other planned wind energy application sites.

### Censoring Data

```{r HcoastMaxDist}
# Identify extreme value
HCoast_max <- df_turbine[which(df_turbine$HCoast == max(df_turbine$HCoast)),]
HCoast_max_dist <- HCoast_max$HCoast %>% round(0)
HCoast_max_sitename <- paste(HCoast_max$Site.Name, HCoast_max$County, HCoast_max$Country, sep = ", ")
HCoast_max_dev <- floor(max(df_turbine$HCoast) / mean(df_turbine$HCoast))
```

To mitigate any impact from these extreme values, the data was censored to the maximum value based on current development densities. Compared to truncation, whereby the data points are removed from the model, censoring retains the value but limits it a maximum value. It was calculated that 80km was the furthest any site is from a developed wind energy, and this value was therefore set as the maximum value. 

Further concern was raised for the scale of distance considered for geographic features. As an example, the furthest site from a Heritage Coast designation is `r HCoast_max_dist`km (this site being `r HCoast_max_sitename`). As would be expected, the planning decision^[See https://goo.gl/mUA7QL for the planning decision documentation] for this site made no reference to the Heritage Coast, as clearly a site would not be influenced by a landscape designation so far away. Censoring was therefore applied to the proximity values within the dataset, using the maximum distance of 30km, which is referenced as the greatest distance that wind turbines are perceived to have a visual impact [@Guidance2008].

Two exceptions were made within the censoring process:

1. **Airports**: there is generally a greater impact caused by wind turbines due to flight paths and radar [@CAA2016], and therefore no censoring was applied to turbine data. 
2. **Nearest Turbine**: literature suggests that regional scale perceptions to wind energy can be influenced by wind turbines, even if it is not visible from the other site [@Eltham2008]. The value was therefore left at the 80km limit set after the removal of outliers previously discussed.


### Collinearity

Collinearity between predictor variables can impact on the fit of models, resulting in inflated standard errors of regression coefficients and reducing the power of corresponding tests [@Harrell2001, pg.64]. As a result, checks are recommended to be made both before and after the model has been built. A correlation matrix is shown in Figure \@ref(fig:CorrelationMatrix) which displays the pairwise Pearson product-moment correlation coefficient, and provides a means to visually inspect potential collinearity. The results indicate a number of key relationships:

```{r CorrelationPlot}

turbines_cor <- turbines_numeric %>%
  matchNamesColumns() %>%
  cor(use="pairwise") %>% 
  round(2)
diag(turbines_cor) <- NA

# Produces a list of the correlation results
zdf <- as.data.frame(as.table(turbines_cor))
zdf <- subset(zdf, abs(Freq) > 0.5)

FindCorrelation <- function(x,y, corrmatrix){
  col <- which(names(corrmatrix) == x)
  row <- which(rownames(corrmatrix) == y)
  return(corrmatrix[x,y] %>% round(2))
}
```

- **Number of Turbines** and **Capacity** (R^2^ = `r FindCorrelation("Number of Turbines", "Capacity", turbines_cor)`): unsurprisingly, this suggests that wind farms with more turbines will have more overall capacity.
- **Qualifications** and **Social Grade** (R^2^ = `r FindCorrelation("Qualifications, L4", "Social Grade AB", turbines_cor)`): this suggests that those who attain higher professional positions would typically have higher levels of qualifications.
- **A number of proximity features**: many features (powerlines, roads, railways, military sites) are indicated to have a high level of correlation.
- **Grouping of linked parameters**: A number of parameters were split or derived from the same datasets which would be expected to be highly correlated. These include 1) Small, Large and All Urban Areas 2) Nearest Turbines All, Nearest Turbine Planned, Nearest Turbine Rejected.


(ref-CorrelationMatrix) "Bivariate correlation matrix for predictor variables within the analysis"

```{r CorrelationMatrix, fig.height=11, fig.width= 10, fig.cap = "(ref-CorrelationMatrix)"}
corrplot::corrplot(turbines_cor, type="upper", order="hclust", tl.col = "black", na.label = " ", number.cex=0.01)
```

Whilst these correlations provide an important insight into the dataset, it is not recommended that such observations are used as the sole justification for removing parameters from the model. However, these results raise concerns which should be explored further, and therefore additional assessments of multicollinearity were made within the statistical analysis, as presented in Section \@ref(TurbineStatisticalAnalyis).

### Linearity of Logit

As briefly discussed within the influential outliers section, logistic regression assumes linearity between the predictor variable and the logit [@Harrell2001]. Assessments were therefore conducted to assess this assumption, with smoothed scatter plots shown in Figure \@ref(fig:LinearityPlots)^[Note: the graph uses the censored datasets as described within this chapter. Individual observations are shown in the rug plots which highlight the marginal distributions], whereby each point represents an aggregated percentile. Linearity can be assessed by the shape of the fitted curve, and the magnitude of the potential influence is indicated by the gradient of this fit. The following general observations were made:

(ref:LinearityPlotsCap) Scatter plots showing proportion of sites receiving planning against each predictor variable.

```{r LinearityPlots, fig.width=10, fig.height=13, message=FALSE, warning=FALSE, fig.cap="(ref:LinearityPlotsCap)", fig.pos="p", out.width = "100%"}

df_turbine_trunc <- read.csv("turbine_proximity_full.csv")

# Createa a subset of the parameters
namelist <- c("Airports", "AONB_Trans", "ARoads_Trans", "BRoads_Trans", "HCoast_Trans",
              "HVpowerline_Trans", "MilitarySites_Trans", "NationalParks_Trans", "NNR_Trans", "Powerlines_Trans",
              "PrimaryRoads_Trans", "Railway_Trans", "RAMSAR_Trans", "SACS_Trans", "SPA_Trans",
              "SSSI_Trans", "UrbanLarge_Trans",  "UrbanSmall_Trans", "NearestTurbineBuilt",
              "NearestTurbinePlanned", "NearestTurbineRejected", "Capacity", "Turbine.Capacity..MW.", "LD_share", "Lab_share",
              "Con_share", "SNP_PC_sha", "QualPercL4", "SocGrdAB", "PercOwn", "AgeMean", "UKElevation", "Slope", "WindSpeed45")

plots <- lapply(namelist, function(x) ScatterPlotOdds(df_turbine_trunc, variable = x,quantiles =  25))
do.call(gridExtra::grid.arrange, c(plots, ncol=4))
```

- **Linearity of relationships**: most proximity features appear to have a linear relationship (AONBs, National Parks, Military, Natura 2000 sites). 
- **Non-linearity**: Notable examples include Liberal Democrat voter share, and distance to urban areas.
- **Social Grade, Mean Age and Qualifications**: all three of these variables appear to negatively influence the planning acceptance rates. Combined with the high levels of correlation, these again may suggest multi-collinearity between the variables.
- **Overfitting of curves**: although efforts were made to remove influential outliers, some graph appear influenced by non-linear relationships (e.g. Heritage Coast, Small Urban Areas, Primary Roads). 

Whilst transformations can be used to correct non-linear relationships, it is recommended that these are only made with a valid hypothesis: attempting to make transformations for the purpose of improving model fit can introduce type I errors and also limit the interpretability of the resulting model [@Harrell2001]. It was observed within existing GIS wind turbine studies that linear relationships are primarily used. As a result, it was decided that no transformations were to be made to the dataset.

Again, while these results can help indicate potential issues, it is not recommended that parameters are removed prior to the analysis based on these results. There is the potential that the interaction with other variables within the model may alter these relationships, and therefore further linearity checks are conducted within the diagnostics of the statistical modelling.

----

# Step 2: Hierarchical Logistic Regression

Summary statistics provide a useful overview of the modelling data, and can provide an indication of issues which may warrant further exploration. Table \@ref(tab:SummaryStatistics) presents the statistics for the numeric variables within the model. It can be seen that the geospatial proximity to features used within the study vary significantly in their range. For example, the maximum distance a wind energy project is from *Urban Regions* is `r max(df_turbine$UrbanRegions) %>% round(., 1)`km while *Heritage Coast* has a range of `r max(df_turbine$HCoast) %>% round(., 1)`km. Such a large difference in range can create difficulty in interpreting results, as a unit change of 1km has a much larger impact on the *Urban Region* parameter than *Heritage Coast*.

While previous studies have transformed the parameters to a standardized scale by using a z-score linear transformation [@VanRensburg20], it was decided that they would be left unaltered within approach: such a transformation provides no direct benefit to the model other than to allow a direct comparison to be made between the influence of parameters. In addition, model parameters create additional complication in the generalisation of the model results as models must transformed to the adjusted z-score scale to allow for comparison [@Harrell2001, p.123].


Some minor amendments were made to the dataset before the modelling to ensure the suitability for modelling:

```{r}
df_turbine <- read.csv("turbine_proximity_full.csv")
```

## Model 1: Physical attributes of the plant

This is based on some of the physical attributes of the wind turbines proposed. This includes the number of wind turbines of the site, and the turbine capacity which can be used to indicate the turbine size. The model initially included the overall wind farm capacity, however it was found that this was this was highly collinear with the number of turbines on the site. The capacity was therefore removed to reduce this influence on the model.

```{r, echo = TRUE}
# Make blank object to store results
wind_models <- NULL
variables_model_1 <- c("No..of.Turbines", "Turbine.Capacity..MW.")
wind_models$No1 <- glm(Status.Summary ~ No..of.Turbines + Turbine.Capacity..MW.,
                       data = df_turbine,
                       family = binomial())


(summary_model1 <- summary(wind_models$No1)) # Print Results
```

```{r, echo = TRUE}
# Check Diagnostics
LogisticDiagnostics(wind_models$No1)
```

The Pseudo $R^{2}$ values highlight the relatively low level of the model. There are no issues with collinearity of the two variables or autocorrelation from the Durbin Watson Test.

```{r, echo = TRUE}
# Check Linearity of variables against logit
LogisticModelInt(variables_model_1, "Status.Summary", df_turbine)
```

The logarithmic transformations are not statistically significant, which suggest that there are no issues with the linearity. 

## Model 2: Developer Parameters

The second model considers parameters which may be of interest to a developer, such as the site wind speed, proximity to powerlines and urban areas.

```{r, echo =TRUE}
# Add variables to parameter List
variables_model_2 <- ParameterUpdate(input = variables_model_1, add = c("WindSpeed45", "HVpowerline"))
wind_models$No2 <- LogisticModel(variables_model_2, df_turbine)
(summary_model2 <- summary(wind_models$No2))
```

```{r}
# Diagnostics
LogisticDiagnostics(wind_models$No2)
```

```{r, echo = TRUE}
# Linearity of variables against logit
LogisticModelInt(variables_model_2, "Status.Summary", df_turbine)
```

Results suggest that HV Powerlines should be checked as they are not linear.

There is a relatively limited improvement in this model compared to the first model. Proximity to powerlines is seen as a significant parameter, however there appear to be issues with its linearity to the logit.

## Model 3: Year of Application

```{r, echo = TRUE}
variables_model_3 <- ParameterUpdate(variables_model_2, add = c( "year"))
wind_models$No3 <- LogisticModel(variables_model_3, df_turbine)
(summary_model3 <- summary(wind_models$No3))
```

## Model 4: Proximity to Features

This model builds upon the previous models to include the full list of geospatial parameters derived in the previous analysis. These provide the proximity to the nearest features and include landscape and environmental designations.


```{r, echo = TRUE}
variables_model_4 <- ParameterUpdate(variables_model_3, add = c( "Airports_Trans", "ARoads_Trans", "BRoads_Trans", "MinRoads_Trans", "Motorways_Trans",  "Railway_Trans", "UrbanRegions_Trans", "AONB_Trans", "NationalParks_Trans", "HCoast_Trans", "NNR_Trans", "RAMSAR_Trans", "SACS_Trans", "SPA_Trans", "SSSI_Trans", "MilitarySites_Trans"))
wind_models$No4 <- LogisticModel(variables_model_4, df_turbine)
(summary_model4 <- summary(wind_models$No4))
```

Check to see whether regression model adequately represents the data. 

```{r}
# Diagnostics
LogisticDiagnostics(wind_models$No4)
```


```{r}
# Check Linearity
LogisticModelInt(variables_model_4, "Status.Summary", df_turbine)
```

Areas of Outstanding Natural Beauty (AONB), National Parks and SPAs are statistically significant environmental and landscape designations. However it appears that there may be issues with AoNB linearity.

## Model 5: Census Variables

This model adds Census data to understand whether demographic variables can be linked to the turbines. Qualifications, Age, Social Grade and Tenure were added to the model.

```{r}

variables_model_5 <- ParameterUpdate(variables_model_4, add = c("QualPercL4", "AgeMean", "TenureOwned"))

wind_models$No5 <- LogisticModel(variables_model_5, df_turbine)
(summary_model5 <- summary(wind_models$No5))

```

Check to see whether regression model adequately represents the data. 

```{r, echo = TRUE}
# Diagnostics
LogisticDiagnostics(wind_models$No5)
```

```{r}
# Linearity
LogisticModelInt(variables_model_5, "Status.Summary", df_turbine)
```

Conclusions:

- Increased levels of qualification appear to reduce the likelihood of acceptance
- Age_Median and Mean are highlight collinear. Median value removed

## Model 6: Political Parameters

This adds information from local authority composition. Represents the complete model presented within the report.

```{r, echo=TRUE}
variables_model_6 <- ParameterUpdate(variables_model_5, add = c("Con_share", "Lab_share", "LD_share"))
# Could add "SNP_PC_sha"
wind_models$No6 <- LogisticModel(variables_model_6, df_turbine)
(summary_model6 <- summary(wind_models$No6))

```


```{r, echo =TRUE}
# Diagnostics
LogisticDiagnostics(wind_models$No6)
```

## Model 7: Cumulative Development

This includes parameters which indicate how site the planning permission was to other sites at the time of the application.

```{r, echo=TRUE}
variables_model_7 <- ParameterUpdate(variables_model_6, add = c("NearestTurbineBuilt", "NearestTurbineRejected", "UrbanLarge"), remove = "UrbanRegions")
wind_models$No7 <- LogisticModel(variables_model_7, df_turbine)
(summary_model7 <- summary(wind_models$No7))
```
Check to see whether regression model adequately represents the data. 

```{r, echo = TRUE}
# Diagnostics
LogisticDiagnostics(wind_models$No7)
```

```{r}
# Linearity
LogisticModelInt(variables_model_7, "Status.Summary", df_turbine)
```

```{r}
LogisticOddsPlot(wind_models$No7, Title = "Logistic Odds Plot Full Model")
```

# Save Tables

Save results for use within the full report.

```{r LogisticModelsResults}
# Build the results tables to compare the models
list_models <- list(wind_models$No1, wind_models$No2, wind_models$No3, wind_models$No4, wind_models$No5, wind_models$No6, wind_models$No7)

# Run the Above functions
model_results <- LogResultsTable(wind_models$No1, wind_models$No2, wind_models$No3, wind_models$No4, wind_models$No5, wind_models$No6, wind_models$No7)

# Calculate the Model accuracy
foldSize <- 0.05
model_acc <- lapply(list_models, function(x) ModelAccuracyFromModel(x, df_turbine_trunc, iterations = 50, foldSize = foldSize)) %>%
  as.data.frame() %>%
  set_rownames("Accuracy [note]") %>%
  set_colnames(paste("Model", seq(1, length(list_models))))

model_results <- rbind(model_results, model_acc) 

# Make footnote show how accuracy was derived
footnote <-  paste0("Accuracy assessed by internal validation using a random sample of ", foldSize*100, "% with 200 iterations.")
  
knitr::kable(model_results, 
             format = "latex",
             caption = "A summary of the hierarchical logistic regression models",
             booktabs=TRUE,
             linesep = "") %>%
  kableExtra::kable_styling(latex_options = c("hold_position", "scale_down")) %>%
  kableExtra::add_footnote(label = footnote)
```

```{r}
# save tables for report
write.csv(model_results, file = "outputs/modelResults.csv")

```

## Parsimonious Model

```{r}
df_turbine <- read.csv("turbine_proximity_full.csv")

Dataframe <- df_turbine_trunc[, match(c("Status.Summary", variables_model_7) ,names(df_turbine_trunc))]
df_turbine <- Dataframe[complete.cases(Dataframe),]

# Rebuild the general model. There is an issue with `step` working with models produced using formula

fullModel <- glm(Status.Summary ~ No..of.Turbines + Turbine.Capacity..MW. + WindSpeed45 + 
    HVpowerline + year + Airports_Trans + ARoads_Trans + BRoads_Trans + 
    MinRoads_Trans + Motorways_Trans + Railway_Trans + UrbanRegions_Trans + 
    AONB_Trans + NationalParks_Trans + HCoast_Trans + NNR_Trans + 
    RAMSAR_Trans + SACS_Trans + SPA_Trans + SSSI_Trans + MilitarySites_Trans + 
    QualPercL4 + AgeMean + TenureOwned + Con_share + Lab_share + 
    LD_share + NearestTurbineBuilt + NearestTurbineRejected + 
    UrbanLarge, data = df_turbine, family = stats::binomial())


model_pars <- stats::step(fullModel, direction = "backward", trace = 0)
variables_pars <- broom::tidy(model_pars)[-1, 1] 
model_pars_Summary <- LogResultsTable(model_pars)
```





```{r BuildNestedModels, include=FALSE}
variables_pars <- variables_pars[variables_pars != "SNP_PC_sha"]

# Uses a range of custom functions within "Segmented_LogisticModelsComplete" to split data
segmented_country <- Segmented_LogisticModelsComplete(df_turbine_trunc, "Country", variables_pars, "Status.Summary", 0.2, 1)

max_wales_variables <- table(segmented_country$data$Wales$Status.Summary) %>% min() %>% divide_by(10) %>% floor

# Build Optimised Parameter Models
models_split <- NULL
models_split$England <- FindBestModel(df = segmented_country$data$England, variables_pars)
models_split$Scotland <- FindBestModel(df = segmented_country$data$Scotland, variables_pars)
models_split$Wales <- FindBestModel(df = segmented_country$data$Wales, variables_pars)


# Summarise the results
summary_split_model <- LogResultsTable(model_pars, 
                                         segmented_country$models$England,
                                         segmented_country$models$Scotland,
                                         segmented_country$models$Wales,
                                         models_split$England,
                                         models_split$Scotland,
                                         models_split$Wales) %>%
  set_colnames(c("Global", "England", "Scotland", "Wales", "England", "Scotland", "Wales"))

summary_split_model <- summary_split_model[c(1:8),] # Deletes the final AIC row

# Calculate Accuracy
# The fold size for wales must be increased as there is a very small sample size
accuracy <- c(ModelAccuracyFromModel(model_pars, df_turbine_trunc),
              ModelAccuracyFromModel(segmented_country$models$England, segmented_country$data$England),
              ModelAccuracyFromModel(segmented_country$models$Scotland, segmented_country$data$Scotland),
              ModelAccuracyFromModel(segmented_country$models$Wales, segmented_country$data$Wales, foldSize = 0.15),
              ModelAccuracyFromModel(models_split$England, segmented_country$data$England),
              ModelAccuracyFromModel(models_split$Scotland, segmented_country$data$Scotland),
              ModelAccuracyFromModel(models_split$Wales, segmented_country$data$Wales, foldSize = 0.15)) %>%
  as.data.frame() %>%
  t() %>%
  set_colnames(names(summary_split_model)) %>%
   set_rownames("Accuracy")

# Calculate values to quote in text
summary_split_model <- rbind(summary_split_model, accuracy)
num_params <- summary_split_model['Parameters',] %>% t


write.csv(summary_split_model, file = "outputs/SplitModels.csv")
```


## Bibliography

```{r include=FALSE}
knitr::write_bib(x =c("tidyverse", "caret", "corrplot", "ggplot2", "magrittr", "Hmisc", "broom", "car", "plyr", "psych", "stats", "ggthemes", "grid", "DiagrammeR", "rsvg", "DiagrammeRsvg", "scales", "knitr", "rmarkdown", "WindAnalysis"), file = "packages.bib")
```
