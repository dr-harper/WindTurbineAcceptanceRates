# Wind Turbine Statistical Assessment {#TurbinesStats}

```{r Methodology 3}
knitr::include_graphics("_figures/Methodology/PhDMethodology_3.pdf")
```

As highlighted within Section \@ref(WindStudies), there have been many GIS-MCDA models which have been developed within existing literature [@Watson2015; @Baban2001; @Voivontas1998]. These utilise a range of geospatial parameters to identify suitable sites for wind energy development. However, there is a limited understanding of the input assumptions which are largely based on empirical evidence, and as a result concerns have been raised regarding the validity and accuracy of existing approaches [@Langer2016]. This chapter presents statistical analysis which investigates the relationship between planning decision outcomes and a of range explanatory variables included with literature, and has the following objectives:

- to validate the suitability of modelling data for statistical modelling.
- to make suitable adjustments to the model data to correct for potential issues within the modelling.
- to explain the iterative process used to conduct the statistical modelling.
- to identify the key influential parameters within the acceptance rate.
- to determine the overall fit of the statistical model for predicting acceptance rates.
- to generalise the findings of the model to predict the acceptance rates of wind energy projects.

### Chapter Structure {.unnumbered}

As highlighted in Figure \@ref(fig:StatStages), the chapter is formed of three key parts:

1. **Preliminary Data Analysis** conducts a number of preliminary checks to the dataset. This aims to provide an insight into the dataset and to ensure that the datasets are suitable for the statistical analysis.
2. **Statistical Analysis** explains the statistical modelling used to assess the acceptance rates of wind energy planning applications. Four different modelling approaches were developed to determine a suitable model.
3. **Generalisation of Results:** utilises the results from the statistical analysis, and explains the process of generalising the model so that it can be used to predict the likelihood of acceptance of projects.

```{r StatStages, fig.cap="Stages of Statistical Analysis", out.width = "100%"}
include_graphics("_figures/StatsAnalysis.pdf")
```

Following each section, a discussion concludes the key points. The chapter aims to highlight the development process which shaped the analysis, and explain the rationale for the decisions made within the modelling.

\newpage
## Preliminary Data Analysis {#DataChecks}

```{r LoadStatsData}
data_turbines_full <- read.csv("../data/process/3_TurbineDatasetExtraction/TurbineFullInfo.csv", header = TRUE)

# Removes "Awaiting Decision" Factor from list
data_turbines <- filter(data_turbines_full, Status.Summary != "Submitted")
data_turbines$Status.Summary <- factor(data_turbines$Status.Summary, levels=c("Refused/Abandoned","Approved"))
data_turbines$Planning_Status_Code <- (data_turbines$Status.Summary %in% "Approved") %>% multiply_by(1)
data_turbines <- filter(data_turbines, Size == "Large")

# Variables used in model
variables <- read.csv("../data/input/StatisticalVariables.csv")
model_variables <- as.character(variables$Variable)
```

It has been noted that statistical models are fundamentally influenced by the quality of the data used to build them [@Harrell2001, p.26]. Failing to understand any limitations within the dataset can result in misspecified statistical models. It is therefore critical that checks are conducted to fully understand any potential issues within input parameters before modelling, and that any assumptions required for the statistical analysis are met. This section therefore presents the preliminary analysis conducted to understand the dataset.

### Summary Statistics

```{r CreateSummaryTable}
turbines_numeric <- matchNamesColumns(data_turbines) %>% FiltNumericColumns()
turbines_numeric <- turbines_numeric[,!(names(turbines_numeric) %in% c("RO Banding", "NA", "NA.1", "NA.2", "NA.3", "NA.4", "NA.5", "NA.6", "Y Coordinate", "X Coordinate"))] %>% set_rownames(NULL)# Delete RO banding category
summaryTable <- SummariseDataframe(turbines_numeric)[-1,]
```

Summary statistics provide a useful overview of the modelling data, and can provide an indication of issues which may warrant further exploration. Table \@ref(tab:SummaryStatistics) presents the statistics for the numeric variables within the model. It can be seen that the geospatial proximity to features used within the study vary significantly in their range. For example, the maximum distance a wind energy project is from *Urban Regions* is `r max(data_turbines$UrbanRegions) %>% round(., 1)`km while *Heritage Coast* has a range of `r max(data_turbines$HCoast) %>% round(., 1)`km. Such a large difference in range can create difficulty in interpreting results, as a unit change of 1km has a much larger impact on the *Urban Region* parameter than *Heritage Coast*.

While previous studies have transformed the parameters to a standardized scale by using a z-score linear transformation [@VanRensburg20], it was decided that they would be left unaltered within approach: such a transformation provides no direct benefit to the model other than to allow a direct comparison to be made between the influence of parameters. In addition, model parameters create additional complication in the generalisation of the model results as models must transformed to the adjusted z-score scale to allow for comparison [@Harrell2001, p.123].

```{r SummaryStatistics}
knitr::kable(summaryTable, booktabs = TRUE, caption = "Summary statistics of the parameters collected for onshore wind energy within the statistical analysis. These values represent the dataset before missing values were imputed") %>%
  kableExtra::kable_styling(latex_options = "scale_down") %>% 
  kableExtra::add_footnote("Political data represents the number of seats held within the local council, not the vote share. This explains why the range can vary between 0 and 100")
```

### Missing Values

```{r MissingValues}
observation_count <- nrow(data_turbines)
percent_missing <- (1 - (complete.cases(turbines_numeric[names(turbines_numeric) != "Political, SNP Share"]) %>% multiply_by(1) %>% sum)/nrow(data_turbines)) %>% multiply_by(100) %>% round(1)
```


Logistic regression cannot directly deal with missing values within the dataset, and if an observation lacks any single parameter, the whole record will be removed from the data. This can result in a loss of data and potential impact on the modelling results, as a smaller sample size is used to build the model. It is therefore recommended that missing values are assessed, and where appropriate, imputed to create complete records [@Harrell2001, @Griffith2003].

It can be seen within the summary table that there are few cases of missing data across the total dataset (n =`r observation_count`), with a total of `r percent_missing`% observations being incomplete. It was found that Slope, Elevation and Wind Speed missing data was a result of the rasterisation of the input dataset, with sites near the edge of land (rivers, coastline, lakes etc.) being occasionally lost within the dataset. Further missing data was seen within the political data, which resulted from boundary changes in 1995 which made it not possible to map, and the SNP party only exist within Scotland. 

Two techniques were used to impute missing datasets [@MandelJ2015]. Firstly, mean substitution was used to impute missing slope, elevation and political data. For the SNP data, any missing values which were in England and Scotland were imputed as 0. 

```{r ImputeMissingValues}
# Fill missing values
MissingCols <- c("Slope", "UKElevation", "WindSpeed45", "Con_share", "LD_share", "Lab_share", "Oth_share")

for(i in MissingCols){
  data_turbines[[i]] <-Hmisc::impute(data_turbines[[i]], what = mean)
}

data_turbines$SNP_PC_sha <- Hmisc::impute(data_turbines$SNP_PC_sha, 0) %>% as.numeric()
```

### Influential Outliers

Influential outliers are observations where values deviate from the expected range and produce extremely large residuals [@Hosmer2004]. These can result in incorrect inferences from the statistical model, as the model may overfit to these extreme cases. It is therefore recommended that assessments are made before statistical modelling is conducted to identify potential cases which may need removal. 

Figure \@ref(fig:plotNearestTurbine) a) highlights the distribution of proximity variables, and provides a visual method to identify potential errors. The boxplots highlight that many parameters have outliers, which are largely caused by a narrow interquartile range resulting from the low standard deviations within the dataset. As the outliers are relatively evenly spread throughout the dataset, with few extreme values, they are of limited concern within the modelling.

```{r plotNearestTurbine, message=FALSE, warning=FALSE, fig.cap="Boxplot for proximity variables which were derived from model parameters. A different censoring distance was applied to Spatial and Spatiotemporal datasets (30km and 80km respectively)"}

data_turbine_boxplots <- matchNamesColumns(data_turbines)

## Identify which columns have Distance parameters
colref <- which((substr(names(data_turbine_boxplots),1 ,8) %in% c("Distance")) == TRUE) 
data_turbine_boxplots <- data_turbine_boxplots[, colref] %>% melt() %>% mutate(id = "a) Spatial")
data_censored <- filter(data_turbine_boxplots, value > 30)
data_turbine_boxplots2 <- matchNamesColumns(data_turbines)

## Identify which columns have Distance parameters
colref <- which((substr(names(data_turbine_boxplots2),1 ,8) %in% c("Nearest ")) == TRUE) 
data_turbine_boxplots2 <- data_turbine_boxplots2[, colref] %>% melt() %>% mutate(id = "b) Spatiotemporal")
data_censored <- filter(data_turbine_boxplots2, value > 30)

lines <- data.frame(id = c("b) Spatiotemporal", "a) Spatial"), value = c(80, 30))

ggplot(rbind(data_turbine_boxplots2,data_turbine_boxplots), aes(x = reorder(variable, value, FUN = median), y = value)) +
  geom_boxplot(aes(fill = reorder(variable, value, FUN = median)), outlier.shape = 1, outlier.size = 0.5, outlier.alpha = 0.6) +
  scale_fill_discrete(grDevices::rainbow(20)) +
  scale_y_continuous(limits=c(0, 400)) +
  coord_flip() +
  theme(legend.position="none") +
  labs(x = "", y = "Distance, (km)") +
  geom_hline(data = lines, aes(yintercept = value), linetype = 2, col = "grey10", alpha = 0.5) +
  facet_grid(id~., scales = "free_y", space = "free_y") +
  theme(strip.text.y = element_text(angle = 0, colour = "black", hjust = 0),
        strip.background = element_blank())
```

A second boxplot is shown in Figure \@ref(fig:plotNearestTurbine) b) which highlights the "*Nearest Turbine*" parameter dataset. These are represented separately as their derivation used a different approach to the other geospatial proximity parameters, considering both the *location* and the *year* of the development. This has resulted in a small number of extreme outliers which represent sites which were the first to be developed within a region. Only `r filter(data_turbines, NearestTurbinePlanned>100) %>% nrow` sites were further than 100km from other planned wind energy application sites.

### Censoring Data

To mitigate any impact from these extreme values, the data was censored to the maximum value based on current development densities. Compared to truncation, whereby the data points are removed from the model, censoring retains the value but limits it a maximum value. It was calculated that 80km was the furthest any site is from a developed wind energy, and this value was therefore set as the maximum value. 

```{r Proximitytosite, eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
# Calculates how far sites are from turbine planning areas
# Code not run when knit but left in for reference
data_turbines_spdf_bng <- readOGR(dsn = "../data/process/2_GeospatialData/REPD", layer = "Turbines", verbose = FALSE) %>% spTransform(., crs_bng)
tempRaster <- raster(extent(StudyExtent), nrow = 1000, ncol = 1000)
dist_turbine <- rasterize(x = data_turbines_spdf_bng, y = tempRaster, field = 1) %>% distance()
dist_turbine_cropped <- raster::mask(dist_turbine, StudyExtent)
cellStats(dist_turbine_cropped$layer, stat = "max")
rm(tempRaster, dist_turbine, dist_turbine_cropped)
```

```{r CensorTurbineData}
# Truncate the turbine proximity data to 100km
distance_trunc <- 80
data_turbines_trunc <- data_turbines

for(i in c("NearestTurbineBuilt", "NearestTurbinePlanned", "NearestTurbineRejected")){
  data_turbines_trunc[[i]] [data_turbines_trunc[[i]] > distance_trunc] <- distance_trunc
}

# Save the edited data to file
write.csv(x = data_turbines_trunc, "../data/process/3_TurbineDatasetExtraction/TurbineFullInfoTruncated.csv")
```

```{r HcoastMaxDist}
# Identify extreme value
HCoast_max <- data_turbines[which(data_turbines$HCoast == max(data_turbines$HCoast)),]
HCoast_max_dist <- HCoast_max$HCoast %>% round(0)
HCoast_max_sitename <- paste(HCoast_max$Site.Name, HCoast_max$County, HCoast_max$Country, sep = ", ")
HCoast_max_dev <- floor(max(data_turbines$HCoast) / mean(data_turbines$HCoast))
```

Further concern was raised for the scale of distance considered for geographic features. As an example, the furthest site from a Heritage Coast designation is `r HCoast_max_dist`km (this site being `r HCoast_max_sitename`). As would be expected, the planning decision^[See https://goo.gl/mUA7QL for the planning decision documentation] for this site made no reference to the Heritage Coast, as clearly a site would not be influenced by a landscape designation so far away. Censoring was therefore applied to the proximity values within the dataset, using the maximum distance of 30km, which is referenced as the greatest distance that wind turbines are perceived to have a visual impact [@Guidance2008].

Two exceptions were made within the censoring process:

1. **Airports**: there is generally a greater impact caused by wind turbines due to flight paths and radar [@CAA2016], and therefore no censoring was applied to turbine data. 
2. **Nearest Turbine**: literature suggests that regional scale perceptions to wind energy can be influenced by wind turbines, even if it is not visible from the other site [@Eltham2008]. The value was therefore left at the 80km limit set after the removal of outliers previously discussed.

```{r TruncateNearestTurbine}
turbine_proximity <- read.csv("../data/process/3_TurbineDatasetExtraction/turbine_proximity.csv")

# Truncate the long tail of the proximity data. Any relationship greater than 30km is unlikely to influence outcomes
turbine_prox_truncated <- turbine_proximity[-1]
turbine_prox_truncated[turbine_prox_truncated >= 30] <- 30
names(turbine_prox_truncated) <- paste0(names(turbine_prox_truncated), "_Trans")
turbine_prox_truncated <- cbind(Ref_ID = turbine_proximity$Ref_ID, turbine_prox_truncated)
data_turbines_trunc<- merge(data_turbines_trunc, turbine_prox_truncated, by = "Ref_ID")

# Also remove extreme wind turbine sizes
data_turbines_trunc$Capacity[data_turbines_trunc$Capacity > 100] <- NA
data_turbines_trunc$Turbine.Capacity..MW.[data_turbines_trunc$Turbine.Capacity..MW. > 4] <- NA

# Save to file
write.csv(data_turbines_trunc, "../data/process/3_TurbineDatasetExtraction/turbine_proximity_full.csv")
```

```{r RightCensorRasters, eval = params$runcode}
# The rasters must also be truncated for generalisation later
dir_rasters <- "../data/process/4_BaselineExtraction/Rasters" # Raster File Location
raster_names <- names(turbine_proximity[-1]) # Only need to load the proximity rasters
raster_names <- raster_names[!raster_names %in% c("UrbanLarge", "UrbanSmall")] # Cut out urban areas

# Save a new raster layer of all the rasters truncated to the distance of 30km
TruncateRasterList(dir_rasters = dir_rasters, raster_list = raster_names, truncationValue = 30)
```

### Collinearity

Collinearity between predictor variables can impact on the fit of models, resulting in inflated standard errors of regression coefficients and reducing the power of corresponding tests [@Harrell2001, pg.64]. As a result, checks are recommended to be made both before and after the model has been built. A correlation matrix is shown in Figure \@ref(fig:CorrelationMatrix) which displays the pairwise Pearson product-moment correlation coefficient, and provides a means to visually inspect potential collinearity. The results indicate a number of key relationships:

```{r CorrelationPlot}
library(corrplot)

turbines_cor <- cor(turbines_numeric, use="pairwise") %>% round(2)
diag(turbines_cor) <- NA
# Produces a list of the correlation results
zdf <- as.data.frame(as.table(turbines_cor))
zdf <- subset(zdf, abs(Freq) > 0.5)

FindCorrelation <- function(x,y, corrmatrix){
  col <- which(names(corrmatrix) == x)
  row <- which(rownames(corrmatrix) == y)
  return(corrmatrix[x,y] %>% round(2))
}
```

- **Number of Turbines** and **Capacity** (R^2^ = `r FindCorrelation("Number of Turbines", "Capacity", turbines_cor)`): unsurprisingly, this suggests that wind farms with more turbines will have more overall capacity.
- **Qualifications** and **Social Grade** (R^2^ = `r FindCorrelation("Qualifications, L4", "Social Grade AB", turbines_cor)`): this suggests that those who attain higher professional positions would typically have higher levels of qualifications.
- **A number of proximity features**: many features (powerlines, roads, railways, military sites) are indicated to have a high level of correlation.
- **Grouping of linked parameters**: A number of parameters were split or derived from the same datasets which would be expected to be highly correlated. These include 1) Small, Large and All Urban Areas 2) Nearest Turbines All, Nearest Turbine Planned, Nearest Turbine Rejected.

```{r CorrelationMatrix, fig.height=11, fig.width= 10, fig.cap = "Bivariate correlation matrix for predictor variables within the analysis"}
corrplot(turbines_cor, type="upper", order="hclust", tl.col = "black", na.label = " ", number.cex=0.01)
```

Whilst these correlations provide an important insight into the dataset, it is not recommended that such observations are used as the sole justification for removing parameters from the model. However, these results raise concerns which should be explored further, and therefore additional assessments of multicollinearity were made within the statistical analysis, as presented in Section \@ref(TurbineStatisticalAnalyis).

### Linearity of Logit

As briefly discussed within the influential outliers section, logistic regression assumes linearity between the predictor variable and the logit [@Harrell2001]. Assessments were therefore conducted to assess this assumption, with smoothed scatter plots shown in Figure \@ref(fig:LinearityPlots)^[Note: the graph uses the censored datasets as described within this chapter. Individual observations are shown in the rug plots which highlight the marginal distributions], whereby each point represents an aggregated percentile. Linearity can be assessed by the shape of the fitted curve, and the magnitude of the potential influence is indicated by the gradient of this fit. The following general observations were made:

(ref:LinearityPlotsCap) Scatter plots showing proportion of sites receiving planning against each predictor variable.

```{r LinearityPlots, fig.width=10, fig.height=13, message=FALSE, warning=FALSE, fig.cap="(ref:LinearityPlotsCap)", fig.pos="p", out.width = "100%"}
# Createa a subset of the parameters
namelist <- c("Airports", "AONB_Trans", "ARoads_Trans", "BRoads_Trans", "HCoast_Trans",
              "HVpowerline_Trans", "MilitarySites_Trans", "NationalParks_Trans", "NNR_Trans", "Powerlines_Trans",
              "PrimaryRoads_Trans", "Railway_Trans", "RAMSAR_Trans", "SACS_Trans", "SPA_Trans",
              "SSSI_Trans", "UrbanLarge_Trans",  "UrbanSmall_Trans", "NearestTurbineBuilt",
              "NearestTurbinePlanned", "NearestTurbineRejected", "Capacity", "Turbine.Capacity..MW.", "LD_share", "Lab_share",
              "Con_share", "SNP_PC_sha", "QualPercL4", "SocGrdAB", "PercOwn", "AgeMean", "UKElevation", "Slope", "WindSpeed45")

plots <- lapply(namelist, function(x) ScatterPlotOdds(data_turbines_trunc, x, 25))
do.call(grid.arrange, c(plots, ncol=4))
```

- **Linearity of relationships**: most proximity features appear to have a linear relationship (AONBs, National Parks, Military, Natura 2000 sites). 
- **Non-linearity**: Notable examples include Liberal Democrat voter share, and distance to urban areas.
- **Social Grade, Mean Age and Qualifications**: all three of these variables appear to negatively influence the planning acceptance rates. Combined with the high levels of correlation, these again may suggest multi-collinearity between the variables.
- **Overfitting of curves**: although efforts were made to remove influential outliers, some graph appear influenced by non-linear relationships (e.g. Heritage Coast, Small Urban Areas, Primary Roads). 

Whilst transformations can be used to correct non-linear relationships, it is recommended that these are only made with a valid hypothesis: attempting to make transformations for the purpose of improving model fit can introduce type I errors and also limit the interpretability of the resulting model [@Harrell2001]. It was observed within existing GIS wind turbine studies that linear relationships are primarily used. As a result, it was decided that no transformations were to be made to the dataset.

Again, while these results can help indicate potential issues, it is not recommended that parameters are removed prior to the analysis based on these results. There is the potential that the interaction with other variables within the model may alter these relationships, and therefore further linearity checks are conducted within the diagnostics of the statistical modelling.

<!---
# Some useful links
# http://scialert.net/fulltext/?doi=jas.2011.26.35&org=11
--->


```{r StatsSetup, include=FALSE}
# Load Code Specific to this section
source("_scripts/LogisticRegressionSettings.R")
variables <- read.csv("../data/input/StatisticalVariables.csv")
variables_full <- as.character(variables$Variable)
```

### Summary of Preliminary Data Checks

This section has presented the preliminary data analysis conducted on the model data. Several potential issues were raised, with alterations being made to the dataset to ensure the suitability of the model data. In addition, concerns were raised surrounding potential collinearity and the linearity of the logit; however no parameters have been excluded from the analysis on this basis. Further checks will build upon these findings within the following section.

\newpage
## Statistical Analysis of Wind Energy Acceptance Rates {#TurbineStatisticalAnalyis}

Section \@ref(DataChecks) presented the measures taken to ensure the suitability of the model data for further analysis. This section presents the iterative development of the models used, and covers the four following methods:

1. **Hierarchical Model**: this section presents the initial regression model formed by sequentially adding groups of parameters to the model.
2. **Parsimonious Model**: optimisation was run to reduce the number of variables included within the model.
3. **Subset Models**: the model was split into temporal and regional subsets to explore the variance in local parameters within the global model.
4. **Geographically Weighted Regression Model**: building upon the subset models, weighting is applied to the regression modelling to explore for localised impacts.

It should be noted that only a short discussion is provided following each model, which focuses on explaining the rationale of the refinement process. The overall discussion is included within the conclusions in Section \@ref(StatsDiscussion), which reviews the overall model results and discusses influential parameters identified.

### Model Approach {.unnumbered}

Logistic regression analysis was used to assess the influences of a range of predictor variables on the planning acceptance of wind energy projects. This is based on the wind energy planning data collected within the REPD and processed within Chapter \@ref(REPDTurbineData), whereby sites are classed as either rejected (0) or accepted (1). The logistic regression formula is as follows:

\begin{equation}
\text{Probability of Outcome}(Y_{i}) = \ddfrac{e^{\beta_{1}X_{1} + \beta_{2}X_{2} + b_{3}X_{3} +... + b_{j}X_{j}}}{1+ e^{\beta_{1}X_{1} + \beta_{2}X_{2} + b_{3}X_{3}+... + b_{j}X_{j}}}
\label{eq:logisticregression2}
\end{equation}

where $Y_{i}$ represents the estimated probability of being in one binary outcome category $i$ versus the other, and $e^{\beta_{1}X_{1} + \beta_{2}X_{2} + b_{3}X_{3}+... + b_{j}X_{j}}$ represents the linear regression equation for independent variables expressed in the logit scale, instead of the original linear format.

For each model developed within the analysis, the suitability was assessed and diagnostics conducted, including the Hosmer-Lemeshow goodness of fit test [@Hosmer2004]. Model parameters were assessed for linearity of the logit, and collinearity identified using the Variance Inflation Factor (VIF). Any parameters which violated the assumptions of logistic regression modelling were removed from the analysis to prevent distortion of the model.

To further validate the model fit, the model's accuracy was assessed using K-fold internal validation. This approach splits the data into a $K$ number of subgroups, with $(K-1)$ number of groups being used to "train" the model, while the remaining group is used to "test" the model. This process is repeated for each of the subgroups, allowing for a resilient test which can be used to validate the model without excessively reducing the building sample size [@Hosmer2004].

The reduced results are presented within the body of the thesis, with full results provided within Appendix \@ref(LogRegDetailed). A brief primer of the theory and key concepts of logistic regression is included in Appendix \@ref(StatsPrimer).

### Hierarchical Logistic Regression Model {#HierarchicalLog}

It was shown in Section \@ref(WindStudies) that existing literature placed an emphasis on physical characteristics when assessing the suitability of a site for wind energy development. However, qualitative studies have indicated that socio-demographic parameters are important within the acceptability of wind energy [@Langer2016], although to the authors knowledge, no GIS models have integrated these within their analysis directly. 

A hierarchical approach was therefore applied to build the first model, whereby variables were added sequentially to the model. This is suitable for where a hypothesis based on a predetermined order of importance, as it allows for the relative impact of each group of parameters to be assessed [@Harrell2001]. The following hierarchy was therefore developed for the analysis:

1. **Aspatial Site Attributes**: variables including *Number of Turbines* and *Installed Capacity*.
2. **Economic Considerations**: parameters which influence the cost effectiveness of the site, including *Wind Speed* and *Proximity to the National Grid*.
3. **Temporal Aspect**: the year the planning application was made.
4. **Proximity to Features**: inclusion of proximity to geographic features, Landscape and Nature Designations
5. **Social and Census Data**: Demographic data for the area of the wind energy project, including *Mean Age* and *Level of Qualifications*.
6. **Political Data**: the political composition of the local authority composition at the time of the planning application.
7. **Spatial Proximity to Other Turbines**: considers the proximity to the nearest wind energy project in order to assess cumulative impacts.

```{r SourceLogisticRegression, message=FALSE, warning=FALSE, include=FALSE, eval = TRUE}
# Full Analysis is run in the appendix. This executes the code here so that the full results can be viewed.
# This code does result in the Appendix code being run twice, but allows for the full analysis to be called within this document.
ksource("502_App_LogRegHier.Rmd")
```

```{r HoslemTest}
turbines_complete_cases <- data_turbines[complete.cases(data_turbines[,names(data_turbines) %in% variables_model_7]),]
Hoslem_Model7 <- generalhoslem::logitgof(turbines_complete_cases$Status.Summary, fitted(wind_models$No6))
```

#### Results {.unnumbered}

A summary of the stages of the hierarchical model is shown in Table \@ref(tab:LogisticModelsResults). It can be seen that there is a marginal improvement of the Nagelkirke R^2^ values across the results, and that the predictive accuracy marginally improves as more parameters are included within the model. For the Hosmer-Lemeshow test, the final model reports a p-value of `r Hoslem_Model7$p.value %>% round(., 3)`, which indicates that the null hypothesis holds that the model fits the data.

```{r LogisticModelsResults}
# Build the results tables to compare the models
list_models <- list(wind_models$No1, wind_models$No2, wind_models$No3, wind_models$No4, wind_models$No5, wind_models$No6, wind_models$No7)

# Run the Above functions
model_results <- LogResultsTable(wind_models$No1, wind_models$No2, wind_models$No3, wind_models$No4, wind_models$No5, wind_models$No6, wind_models$No7)

# Calculate the Model accuracy
foldSize <- 0.05
model_acc <- lapply(list_models, function(x) ModelAccuracyFromModel(x, data_turbines, iterations = 50, foldSize = foldSize)) %>%
  as.data.frame() %>%
  set_rownames("Accuracy [note]") %>%
  set_colnames(paste("Model", seq(1, length(list_models))))

model_results <- rbind(model_results, model_acc) 

# Make footnote show how accuracy was derived
footnote <-  paste0("Accuracy assessed by internal validation using a random sample of ", foldSize*100, "% with 200 iterations.")
  
knitr::kable(model_results, 
             format = "latex",
             caption = "A summary of the hierarchical logistic regression models",
             booktabs=TRUE) %>%
  kableExtra::kable_styling(latex_options = c("hold_position", "scale_down")) %>%
  kableExtra::add_footnote(label = footnote)
```
\vspace{2mm}

The results for the parameters included within the complete model (Model 6) are shown in Table \@ref(tab:LogisticParameterTable), which indicates the statistically significant parameters. The odds ratios (OR) are presented; an OR = 1 indicates the parameter does not affect odds of the planning outcome, an OR > 1 indicate the parameters positively influence planning acceptance, and an OR < 1 represents a negative parameter influence.

```{r LogisticParameterTable}
odds_table <- LogisticResultsTable(wind_models$No7)
odds_table$Variable <- matchNames(odds_table$Variable) # Changes variable names with full values

knitr::kable(x = odds_table, 
             format = "latex",
             booktabs = TRUE, caption = "Parameter results for the complete hierarchical logistic regression model (Model 7)") %>%
  kableExtra::kable_styling(latex_options = "scale_down") %>%
  kableExtra::add_footnote("Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1")
```

#### Model Comments {.unnumbered}

The results presented in Table \@ref(tab:LogisticModelsResults) indicate that the model has a relatively poor overall fit, and that with the full set of parameters included, the Nagelkirke R^2^ value is only `r model_results[['Model 7']]["R.n"] %>% as.character()`. To address these concerns, more advanced logistic regression modelling techniques were explored to improve the model fit, as explained in the following sections.

### Parsimonious Model {#ParameterRefinement}

The hierarchical approach considered the full list of model variables, resulting in a relatively complex model. However, it can be seen that a large proportion of these variables have a low estimate value and statistical significance, and their inclusion provides minimal improvement to the model fit. Before advanced optimisation techniques were considered, the number of model parameters was reduced to create a parsimonious model.^[A parsimonious model is a model that accomplishes a desired level of explanation or prediction with as few predictor variables as possible]

```{r SelectHierModel}
model_hier <- wind_models$No7
variables_hier <- variables_model_7 # reassign name
```

The backward elimination technique was used to develop parsimonious models. In this approach, all candidate variables are included within the model, and the deletion of each variable is tested using a chosen model fit criterion, deleting the variable (if any) whose parameter results in the least deterioration in the model fit. This process is repeated until no further variables can be deleted without a statistically significant loss of fit [@Hosmer2004].

Alternative elimination techniques were considered within the analysis, as there are concerns about the accuracy of backward elimination model in removing parameters. Interaction between parameters may lead to a parameter scoring poorly in the presence of other variables while being significant in a different sub-model [@Harrell2001]. An *"all-subset"* approach was therefore explored, which considers all potential parameter combinations and selects the one with the best fitting. However, it was found to offer no improvement, whilst being computationally intensive to run. The backward elimination approach was therefore deemed suitable for the analysis.

```{r StepwiseModel}
# Build model and filter data
Dataframe <- data_turbines[, match(c("Status.Summary", variables_hier) ,names(data_turbines))]
Dataframe <- Dataframe[complete.cases(Dataframe),]

# Find Best Model
model_pars <- step(model_hier, direction = "backward", trace = 0)
variables_pars <- broom::tidy(model_pars)[-1, 1] 
save(variables_pars, file= "../data/process/6_Statistics/ResultParametersReduced.RData")
model_pars_Summary <- LogResultsTable(model_pars)
```

#### Results {.unnumbered}

The results for the reduced model are presented in Table \@ref(tab:LogisticResultsBestGLM), and the odds ratios are shown graphically in Figure \@ref(fig:LogOddsPlotPars). The total number of parameters in the parsimonious model was reduced from `r length(variables_hier)` to `r length(variables_pars)`. This has led to a marginal improvement in the AIC from `r model_hier$aic %>% round(0)` to `r model_pars$aic %>% round(0)`. However, this has marginally reduced the model fit, with the Nagelkirke R^2^ value reducing from `r LogResultsTable(wind_models$No7)['R.n',] %>% as.character()` to `r LogResultsTable(model_pars)['R.n',] %>% as.character()`. Finally, the predictive accuracy of the model was determined as `r ModelAccuracyFromModel(model_pars, data_turbines, iterations = 100)`.


```{r LogisticResultsBestGLM}
logtable_reduced <- LogisticResultsTable(model_pars)
logtable_reduced$Variable <-matchNames(logtable_reduced$Variable)
knitr::kable(x = logtable_reduced, format = "latex", booktabs = TRUE, 
               caption = "Results from the parsimonious logistic regression model") %>%
  kableExtra::kable_styling(latex_options = c("hold_position", "scale_down"))

```

<!--- Create caption for odds plot. Cannot put % directly in fig.cap --->
(ref:LogOddsPlotParsCap) Logistic odds plot for the parsimonious model. Variables shown in either green or red are statistically significant positive or negative at 95 CI.

```{r LogOddsPlotPars, echo=FALSE, fig.asp=0.4, fig.cap="(ref:LogOddsPlotParsCap)"}
LogisticOddsPlot(model_pars, Sort = FALSE, Title = "")
```

#### Model Comments {.unnumbered}

The complexity of the model has been greatly reduced, with non-significant parameters being removed from the model. Whilst such a model does not improve the overall fit, the reduced numbers of parameters makes it easier to refine the model further, as explained in the following subsections.

### Split Data Model {#NestedModels}

```{r kableCountryTwoWayFrequency}
twft_country <- TwoWayFrequency("Country", "Status.Summary", "Approved", data_turbines)

# knitr::kable(twft_country, booktab=TRUE, caption="Two Way Frequency Table comparing Country against Planning Status") %>% kableExtra::kable_styling(latex_options = c("hold_position"))
```

It was hypothesised that the parameters influencing the planning acceptance of wind energy may vary between countries within Great Britain. England, Scotland and Wales have varying demographics and population densities^[The population density as of 2013 was England: 413/km^2^, Scotland: 68/km^2^, Wales: 149/km^2^ [@ONS2013].] as well as differing institutional support from national governments, with Scotland placing a greater emphasis on the development of onshore wind [@Smith2016; @Spath2012]. It can also be seen that existing planning acceptance rates are lower in England (`r twft_country$"Percentage Approved"[1]`%) than in Scotland (`r twft_country$"Percentage Approved"[2]`%) and Wales (`r twft_country$"Percentage Approved"[3]`%) [@DECC2016]. However, the global model presented in Section \@ref(HierarchicalLog) was not able to account for any regional variation in relationships.

Where significant variation is expected within subgroups, split data models are recommended, whereby the dataset was segmented into groups based on model variables, and regression models were fitted to each subgroup [@Stoltzfus2011]. Whilst a dummy variable could also be considered, such an approach only captures the *level* effect and not the *slope* effect, and therefore only allows for limited variability between the subgroups. A split data approach was therefore developed using the Country variable, creating three separate models for England, Scotland and Wales respectively. 

It was shown in Section \@ref(ParameterRefinement) that a parsimonious parameter list was developed, which found the most significant variables for the *global* model. However, it was also considered that there may be differing influential parameters between each *local* model. Therefore, two approaches used to select the parameters for the models:

1. **Global Parameter model**: each sub-model used the parsimonious model parameters (n = `r length(variables_pars)`) as developed in Section \@ref(ParameterRefinement).
2. **Optimised Parameter model**: The full list of parameters was provided for each model (n = `r length(variables_full)`). A backward step optimisation was used to eliminate non-influential parameters, similar to the approach used in \@ref(ParameterRefinement).

```{r BuildNestedModels, include=FALSE}
variables_pars <- variables_pars[variables_pars != "SNP_PC_sha"]

# Uses a range of custom functions within "Segmented_LogisticModelsComplete" to split data
segmented_country <- Segmented_LogisticModelsComplete(data_turbines, "Country", variables_pars, "Status.Summary", 0.2, 1)

max_wales_variables <- table(segmented_country$data$Wales$Status.Summary) %>% min() %>% divide_by(10) %>% floor

# Build Optimised Parameter Models
models_split <- NULL
models_split$England <- FindBestModel(df = segmented_country$data$England, variables_hier)
models_split$Scotland <- FindBestModel(df = segmented_country$data$Scotland, variables_hier)
models_split$Wales <- FindBestModel(df = segmented_country$data$Wales, variables_hier)


# Summarise the results
summary_split_model <- LogResultsTable(model_pars, 
                                         segmented_country$models$England,
                                         segmented_country$models$Scotland,
                                         segmented_country$models$Wales,
                                         models_split$England,
                                         models_split$Scotland,
                                         models_split$Wales) %>%
  set_colnames(c("Global", "England", "Scotland", "Wales", "England", "Scotland", "Wales"))

summary_split_model <- summary_split_model[c(1:8),] # Deletes the final AIC row

# Calculate Accuracy
# The fold size for wales must be increased as there is a very small sample size
accuracy <- c(ModelAccuracyFromModel(model_pars, data_turbines),
              ModelAccuracyFromModel(segmented_country$models$England, segmented_country$data$England),
              ModelAccuracyFromModel(segmented_country$models$Scotland, segmented_country$data$Scotland),
              ModelAccuracyFromModel(segmented_country$models$Wales, segmented_country$data$Wales, foldSize = 0.15),
              ModelAccuracyFromModel(models_split$England, segmented_country$data$England),
              ModelAccuracyFromModel(models_split$Scotland, segmented_country$data$Scotland),
              ModelAccuracyFromModel(models_split$Wales, segmented_country$data$Wales, foldSize = 0.15)) %>%
  as.data.frame() %>%
  t() %>%
  set_colnames(names(summary_split_model)) %>%
   set_rownames("Accuracy")

# Calculate values to quote in text
summary_split_model <- rbind(summary_split_model, accuracy)
num_params <- summary_split_model['Parameters',] %>% t

```

#### Results {.unnumbered}

The results of the models are summarised in Table \@ref(tab:kableNestedModelResults), which compares the two sets of split data models compared against the results of the global parsimonious model developed in Section \@ref(ParameterRefinement). There has been a general increase in the fit of the models represented by the Nagelkirke R^2^ values. For the Optimised parameter models, it can be seen that the number of parameters within each model varies between `r min(num_params)` and `r max(num_params)`, as derived from the backward elimination process. 

As the split models have been built using different datasets, there is greater difficulty in directly comparing the diagnostics to assess the model fit. Deviance, chi-sqared values, and residual deviance are all dependent on the size of the datasets. Therefore, the models were compared using the sum of the deviance of the split models, with values of `r summary_split_model["Deviance",2:4] %>% t %>% as.numeric %>% sum %>% round` and `r summary_split_model["Deviance",5:7] %>% t %>% as.numeric %>% sum %>% round` for the *Global* and *Optimised* models respectively.


```{r kableNestedModelResults, warning=FALSE}
knitr::kable(summary_split_model, 
             caption = "Comparison of subset Logistic Regression Models based on the global parameters list", format = "latex",
             booktabs=TRUE) %>%
  kableExtra::kable_styling(latex_options = c("hold_position", "scale_down")) %>%
  kableExtra::add_header_above(c(" " = 2, "Global Parameter" = 3, "Optimised Parameters" = 3))
```
\vspace{2mm}

The influence of each parameter within the model are represented in the odds plots shown in Figures \@ref(fig:plotSegmentedOddCountry). It can be seen that there is variation of the odds ratios between countries, although the splitting of data has resulted in a larger margin of error. These regional differences are expanded upon within the discussion of this section.

```{r plotSegmentedOddCountry, fig.cap="Odds plot for Multilevel regression models split by country in the UK", fig.asp=0.72}
# data_turbines <- filter(data_turbines, Ref.ID != "6188")# Remove a turbine causing issues
# data_turbines$Country <- as.factor(as.character(data_turbines$Country)) 
# Specify Custom Colours
segmented_country$plot + 
  scale_fill_manual(values = c("England" = "#fc8d62", "Scotland" = "#8da0cb", "Wales" = "#e5c494"), guide = guide_legend(title = "Country")) +
  guides(fill = guide_legend(reverse=T))
```

There is evidence of over-fitting which has resulted from splitting the data into subsets. It is recommended that the number of parameters $p$ in a model should not be less than $m/10$ where $m$ is the *limiting sample size* $\text{min}(n_{1}, n_{2})$ [@Harrell2001]. The Wales model has limiting sample size of `r table(segmented_country$data$Wales$Status.Summary) %>% min()`, which therefore should mean that no more than `r table(segmented_country$data$Wales$Status.Summary) %>% min() %>% divide_by(10) %>% floor(.)` should be used within the model. This over-fitting can is shown by the fact that despite a high Nagelkirke R^2^ value (`r LogResultsTable(models_split$Wales)['R.n',]`) there is a low predictive accuracy (`r summary_split_model$Wales[9]`), indicating observed values do not agree with the predicted values.

### Geographically Weighted Regression {#GWR}

```{r Moran_I_Test, message=FALSE, warning=FALSE}
# Create the reduced dataframe
data_turbines_pars <- data_turbines[, match(c("Status.Summary", variables_pars, "lat", "lon", "Y.coordinate", "X.coordinate"), names(data_turbines))]
data_turbines_pars <- data_turbines_pars[complete.cases(data_turbines_pars),]
model_pars <- LogisticModel(variables_pars, data_turbines_pars)
residuals <- residuals(model_pars)
data_turbines_pars$residuals <- residuals

# Rebuild spatial dataframe
data_turbines_spdf <- SpatialPointsDataFrame(coords = cbind(data_turbines_pars$X.coordinate, data_turbines_pars$Y.coordinate), data = data_turbines_pars, proj4string = crs_bng)

# Calculate a matrix of spatial weights
spatialweights <- knearneigh(data_turbines_spdf, k=6, RANN=F) %>%
  knn2nb() %>%
  nb2listw()

# Run the model test
results_moran <- spdep::lm.morantest(model = model_pars, listw = spatialweights)
# results_moran$p.value
```

Standard global modelling techniques cannot be used to detect local variations of the predictor variable. More specifically, the same stimulus provokes the same response in all parts of the study region. It was shown in the previous section that split data models were used to explore differences between subgroups in the model, however these could only capture variation for a predetermined spatial category (i.e. Country). However, there was evidence within literature that the acceptability of wind turbines varies at a sub-national level [@VanderHorst2010]. More advanced techniques were therefore explored to capture these regional variations.

To assess the suitability of the global model, the model residuals were mapped as shown in Figure \@ref(fig:MappedResiduals). In a well-specified model, it would be expected to see that residuals varied randomly within the model, with no spatial correlation. However, there are clusters of high and low residuals, indicating that the model is under and over-predicting in certain regions. The Moran-I test was further used to assess the autocorrelation, and it was found that there was spatial clustering within the model (p = 0.0002) [@R-spdep]. Such autocorrelation invalidates the assumption that residual values are independent of each other [@Hosmer2004], and therefore measures were taken to reduce these issues.

```{r MappedResiduals, fig.cap="Mapped Residuals for the parsimonious logistic regression model", fig.height=6, fig.width=6, message=FALSE, warning=FALSE, cache = params$cache, out.width="70%"}
## Create Quantiles for residual
data_turbines_pars$`Residuals Quantile` <- lsr::quantileCut(data_turbines_pars$residuals, 4) 

# Map Residuals
Basemaps$UKalt +
  coord_map(xlim = c(-12, 5), ylim = c(49, 60)) +
 geom_point(data = data_turbines_pars, aes(x= lon, y = lat, fill =`Residuals Quantile`), 
            size = 1.5,
            colour = "grey30",
            shape = 21,
            stroke = 0.1,
            alpha = 0.7) +
  scale_fill_brewer(type = "div", palette = "RdYlGn") +
  theme_NoAxes + 
  theme(legend.position="right",
         panel.border = element_rect(colour = "grey", fill=NA, size=0.5))

```

(ref:GWRfootnote) For a brief primer on geographically weighted regression, the author recommends Kasner et al. [-@Kasner2013].
(ref:GWRfootnote2) spatial nonstationarity is defined as a condition in which a simple "global" model cannot explain the relationships between some sets of variables [@Brunsdon1998].

To address the concerns of spatial nonstationarity^[(ref:GWRfootnote2)] within the data, Geographic Weighted Regression (GWR) was used to account for the spatial location of points within the model [@Brunsdon1998]. This technique builds upon traditional regression modelling, and instead of fitting a single, global regression model, local regression coefficients are fitted to explore geographic differences in estimates across the study region. These local coefficients are obtained at each location in the model, with a regression model applied, whereby sites closer to the location have a greater influence than those further away.^[(ref:GWRfootnote)]

The relationship between the proximity of neighbouring site and the influence which they have on the model is specified by a gaussian kernel function, which requires a bandwidth to be specified. This can be either a fixed width or adaptive, whereby the bandwidth varies for each model to include a prespecified proportion of the observations, as demonstrated in Figure \@ref(fig:Bandwidth). As there is varying spatial density of wind energy projects within Great Britain, an adaptive width bandwidth was selected. This ensures that there are sufficient observations to produce a robust model, even in locations where there are few existing observations [@Kasner2013].

(ref:GWRDiagram) Spatial weighting functions with adaptive Bandwidth. Adapted from Fothering et al. [-@Fotheringham2002].
```{r Bandwidth, fig.cap='(ref:GWRDiagram)', out.width="65%"}
knitr::include_graphics("_figures/gwrkernal.jpg")
```

```{r GWRSetup, include=FALSE}
# spgwr used for GWR modelling
library(spgwr)
# Build reduced model list
importance <- varImp(model_pars) %>% mutate(names = rownames(.)) %>% .[order(.$Overall),]
model_reduced <- importance$names
model_reduced <- model_reduced[model_reduced != "Railway_Trans"]

# Build spatial dataset
data_turbines_spdf <- SpatialPointsDataFrame(coords = cbind(data_turbines$X.coordinate, data_turbines$Y.coordinate), data = data_turbines, proj4string = crs_bng)

# Reduce DF to only include complete cases
data_turbines_spdf_red <- data_turbines_spdf[,names(data_turbines_spdf) %in% c(model_reduced, "Status.Summary", "Planning_Status_Code")]
data_turbines_spdf_red <- data_turbines_spdf_red[complete.cases(data_turbines_spdf_red@data),]

# Relevel Factors
data_turbines_spdf_red$Status.Summary <- factor(data_turbines_spdf_red$Status.Summary, levels=c("Refused/Abandoned","Approved")) # 
```

```{r BuildGWRModels, message=FALSE, warning=FALSE, include=FALSE}
library(GWmodel)
# Alternative metthod for caclulating GWmodel
distmatrix <- gw.dist(dp.locat = data_turbines_spdf_red@coords, rp.locat = data_turbines_spdf_red@coords)
gwr_Formula <- formula(paste("Planning_Status_Code ~ ", paste(model_reduced, collapse=" + ")))
# Create Bandwidth. Not run each time as takes a while! Uncomment to update
# bw <- ggwr.sel(formula = gwr_Formula2, family = binomial(link = "logit"), data = data_turbines_spdf_red@data, coords = data_turbines_spdf_red@coords, verbose = FALSE, adapt = TRUE)
bw <-0.41
GWRmodel <- spgwr::ggwr(formula = gwr_Formula, 
                        data = data_turbines_spdf_red, family = binomial(link = "logit"), adapt = bw)

# Two GWR methods are used, as they return different statistic values
gwr_Formula2 <- formula(paste("Planning_Status_Code ~ ", paste(model_reduced, collapse=" + ")))
bw2 <- (bw * nrow(data_turbines)) %>% floor # bw manaully specified
GWRmodel2 <- GWmodel::ggwr.basic(formula = gwr_Formula2, data = data_turbines_spdf_red, bw = bw2, family = "binomial", adaptive = TRUE)

```

In order to determine the bandwidth size for the model, an optimisation is used whereby the bandwidth selection minimises the overall AIC of the model. For the parsimonious model parameters determined within section \@ref(ParameterRefinement), an optimum adaptive bandwidth of `r bw` was calculated. This value represents the fraction of sites to be used, which equates to `r floor(bw * nrow(data_turbines_spdf_red))` observations per locally weighted model.

```{r GWRMatchPercent}
# Yhat is the predictive value
acc_GWR <- data.frame(Estimate = GWRmodel2$SDF$yhat, Outcome = GWRmodel2$SDF$y)
acc_GWR$Match <- (1 - abs(acc_GWR$Estimate - acc_GWR$Outcome)) %>% round(0)
GWRMatch <- (sum(acc_GWR$Match)/nrow(acc_GWR) * 100) %>% round(0)
```

#### Results {.unnumbered}

The summary results are presented within this chapter while the full statistics are shown in Appendix C. For the GWR model, the corrected Akaike Information Criteria (AICc) is `r GWRmodel2$GW.diagnostic$AICc %>% round(0)`, in comparison with `r model_pars$aic %>% round(0)` from the parsimonious regression model developed previously. The predictive accuracy of the model was estimated as `r GWRMatch`%.

```{r Moran_I_Test_GWR, message=FALSE, warning=FALSE}
# Reapply test
# Calculate a matrix of spatial weights
spatialweights2 <- knearneigh(GWRmodel2$SDF, k=6, RANN=F) %>%
  knn2nb() %>%
  nb2listw()

GWR_moran <- moran.test(x = GWRmodel2$SDF$residual, spatialweights2)
```

The Moran-I test was used to reassess the presence of spatial autocorrelation within the model. There has been a reduction in statistical significance (z = `r GWR_moran$statistic %>% round(2)`, p = `r GWR_moran$p.value%>% round(2)`). However, there remains a high level of autocorrelation within the model residuals, suggesting there may be further unexplained clustering within the dataset.

The local variation of parameters influence is shown in Figure \@ref(fig:PlotGWRResults), which maps the $\beta$ estimates for each model parameter. It can be seen that the global estimate appears suitable for a number of parameters, although there is local variation seen within the model. These will be explored further within the discussion.

```{r FormatGWRcoefficients, message=FALSE, warning=FALSE, include=FALSE}
sample_res <- 5000

# Create a sample of points to used to predict the model
dir_rasters <- "../data/process/4_BaselineExtraction/Rasters" # Raster File Location
raster_names <- variables_pars[!variables_pars %in% c("Turbine.Capacity..MW.", "year", "Railway_Trans")]

# Create Sampling grid
sample_grid <- spsample(x = StudyExtent, type = "regular", cellsize = sample_res) %>% spTransform(crs_bng)

# Extract values from rasters at sample grid
sample_grid$ID <- 1:length(sample_grid)
sample_grid@data <- RasterToPointList(sample_grid, dir_rasters, raster_names, index_col = "ID")

# Fill in non geospatial data
sample_grid@data$"Turbine.Capacity..MW." <- 2
sample_grid@data$"year" <- 2017

# Predict at grid points 
Estimates <- GWmodel::ggwr.basic(formula = gwr_Formula2, data = data_turbines_spdf_red, regression.points = sample_grid, bw = bw2, family = "binomial", adaptive = TRUE)
```

\afterpage{\clearpage}
```{r PlotGWRResults, fig.cap="Results of the Geographically Weighted Regression parameters", fig.width=10, fig.height=15 , message=FALSE, warning=FALSE, fig.pos="p", out.width = "100%", cache = params$cache}
# Solent Plot
StudyExtentPlot <- suppressMessages(StudyExtent %>% spTransform(crs_wgs84) %>% fortify())
StudyExtentPlotBNG <- suppressMessages(StudyExtent %>% fortify())

# GWWRggplotRaster used to create plot
# This function effectively applies a manual facet, as it is desired for each plot to have its own legend
do.call(grid.arrange,
        c(lapply(names(Estimates$SDF)[-1], function(x) ggplot_raster_facet(Estimates$SDF, x)),
          ncol=3))

```

```{r GlobalGWRCompare, eval = FALSE}
# Incomplete analysis
Finally, the results for the global model are compared against the GWR. Figure \@ref(fig:GlobalGWRCompare) indicates the differences between the site estimates against the global regression model generated. 

# Get GWR prediction
GWR_predictions <- GWRmodel2$SDF
GWR_predictions$yhatGlobal <- raster::extract(raster_predicted, GWR_predictions)
GWR_predictions$Diff <- GWR_predictions$yhatGlobal - GWR_predictions$yhat
GWR_predictions <- spTransform(GWR_predictions, crs_wgs84)
ggplot_predictions <- cbind(GWR_predictions@coords, GWR_predictions@data)
Basemaps$UKalt + 
  geom_point(data = ggplot_predictions, aes(x = coords.x1, y = coords.x2, colour = Diff))
spplot(GWR_predictions, "Diff")
```

### Discussion {#StatsDiscussion}

This section has presented the development of the statistical analysis used to assess acceptance rates of wind energy. This discussion aims to summarise the key findings within the model.

#### Overall Model Reflection {.unnumbered}

The development of the hierarchical model was used to help assess the suitability of existing GIS studies. Models 1 and 2 from the initial hierarchical regression models indicate that the physical and economic characteristics are not influential in the acceptance rates of wind energy. This can be seen from both the low Nagelkirke R^2^ values of 0.187, along with the high levels of residuals within the models. The largest improvement was found with the inclusion of the temporal data in Model 3, although the political and demographic data also provided a marginal improvement in model fit.

The parsimonious model highlighted several key parameters, as shown in Figure \@ref(fig:LogOddsPlotPars). Firstly, for project characteristics, the size of the turbine capacity is indicated as a significant parameter, with larger turbines increasing the chance of acceptance. This at first appears counter intuitive, but may suggest that the developers of bigger turbines aremore likely to appeal the decisions made against larger wind farms, as rejection of such projects would result in a large loss of revenue. However, it should be noted that this variable has a small standard deviation (sd = 1.0) compared to other variables included within this model, and therefore the odds ratio inflates their influence within the model.

#### Significant Parameters {.unnumbered}

The distance to urban areas was indicated to be statistically significant, although there is considerable uncertainty as indiciated by the confidence interval. There are a number of potential causes for this: firstly, it could indicate that high wind speed sites suitable for development tend to be naturally less populated (i.e. hilly, isolated regions). Additionally, it may reflect a so-called "Not in My Back Yard" (NIMBY) view from the vocal local population, with projects in closer proximity to urban areas being more likely to be rejected. This has been a relatively contentious subject within literature, with a range of studies supporting [@Haggett2006; @Jones2010a] and rejecting [@VanRensburg20; @Devine-Wright2005a; @Populus2005] the NIMBY argument. However this study provides quantitative evidence to suggest that that sites closer to urban areas have a lower chance of acceptance. 

For landscape and environmental designations, distance to National Parks, Ramsar and AONB were indicated as significant parameters although have marginal impacts. This potentially reflects the negative visual impacts which are often cited as a major impact of wind energy developments [@Langer2016; @Jones2010a]. However, it should be noted that these influences have a relatively low impact, despite literature suggesting that landscape designations would play a more important role.

The level of qualifications, and the mean age of the local populous have been retained as significant parameters for demographic variables. It is suggested that regions of higher education may be more effective in organising campaign groups against such projects. This supports the hypothesis developed by Van der Horst and Toke [@VanderHorst2010] that developers were *"keen to avoid relatively privileged communities and target areas where people are thought to less likely put up a fight"*. To the author's knowledge, such a connection between acceptance rates and demographics has not be previously quantitatively validated.

For political variables, the percentage of local council authority control for Labour appear significant. While there is limited research exploring political views and support for wind turbines, it had been expected that there would be a level of correlation that may result from the Conservative party, as their party has generally oppose the building of wind turbines [@Smith2016]. In addition, other studies have highlighted that voters of Labour and the Liberal Democrats are personally more in favour of onshore wind [@Populus2005], which may result in less local objection against projects in areas where they have stronger support.

The analysis suggests that proximity to existing wind energy developments may influence the likelihood of projects receiving planning. The nearest operational wind energy project was indicated as having a statistically significant negative effect, which suggests that projects further away from an existing project are less likely to be accepted. In addition, the nearest rejected project is suggested to be have a negative estimate, inferring that the further the site is from a previously rejected project, the higher the chance of acceptance. As shown in Section \@ref(AcceptanceParameters), this "proximity hypothesis" has been a contentious subject challenged within literature [@Meyerhoff2010; @Ladenburg2006; @Eltham2008]. However, this study provides quantitative evidence to challenge this view.

There are notable parameters which are frequently used in GIS modelling, but do not prove influential, including wind speed and the proximity to airports. This may reflect that these parameters represent technical challenges which can be investigated in the early stages of project development, and therefore any sites that are not suitable will not seek planning permission. An assessment of these technical limitations forms part of the analysis in Chapter \@ref(GISMCDA).

#### Regional Variations {.unnumbered}

The split data model developed suggests that there are varying influential parameters within England Scotland and Wales, although the reduced number of observations used to build each model increases the uncertainty substantially as indicated by the confidence intervals. Parameters including  *Turbine Capacity*, *Wind Speed* and *Distance to Urban Regions* show differing relationships for each country. This suggests that there may be differing motives for projects within each country as well as differential planning constraints. For example, sites in England appear more sensitive to AONB and National Parks than in Scotland and Wales, and supports research that national level government decisions can influence the local developments [@Langer2016].

The geographically weighted model developed in Section \@ref(GWR) highlights that the local suitability of models should not be overlooked, as there are large amounts of regional variation within the parameter $\beta$ estimates. As an example, the model suggests that demographics such *Mean Age* have a higher influence within the South of England. There is often a desire to build large, national scale models [@Baseer2017], but it is clear that local context is crucial. This supports the literature that indicates that a local understanding is crucial for accurate modelling [@Devine-Wright2005a], and such results could be used to better inform local planning decisions.

Several statistically significant parameters exhibit little regional variation, including the *Distance to Nearest Turbine*, *Year* and *Turbine Capacity*. It is interesting to note that the proximity to turbines holds a global relationship, as it had been suggested in previous studies that regions in South West England were generally more supportive of wind energy if they had existing exposure to such projects [@Jones2011; @Moller2006]. To see that such relationships are valid globally can enable these findings to be used to help inform national policy. 

```{r CleanModel}
# Clean up session
rm(data_turbines_spdf, wind_models, list_models)
```

```{r SetPredicitingSettings, include=FALSE}
# Rebuild the logistic models if required
load("../data/process/6_Statistics/ResultParametersReduced.RData")
variables_pars <- variables_pars[variables_pars != "Railway_Trans"]
model_pars <- LogisticModel(variables_pars, data_turbines)

var_study_year <- 2017  # Year of prediction
var_turbine_size <- 2   # Size of wind turbine

```

\newpage
## Predicting Site Suitability {#StatsGeneralisation}

It was shown in Section \@ref(TurbineStatisticalAnalyis) that a logistic regression model was developed to assess the influence of geospatial characteristics on the success rate of wind energy planning applications. In this section, the results from the analysis were used to spatially forecast the outcome of future planning outcomes.

It should be noted that this section does not consider whether a site is feasibly developable, but only focuses on the likelihood of a project being accepted. This analysis is integrated into a full decision-making model within Chapter \@ref(GISMCDA).

### Model Development

Spatial regression models can be used to generalise the findings of statistical analysis, and are frequently used within geospatial modelling [@Ward2008]. Three main models were developed in the previous section: 1) *global parsimonious* model; 2) *regional* model and 3) *GWR* model. While the GWR model was shown to be the technically most suitable model, there are several concerns surrounding the use of this model for the spatial prediction: 

- For the added complexity, the GWR only led to a marginal improvement in the model accuracy compared to the *global* model.
- The varying $\beta$ coefficient for each parameter make it difficult to compare the results between different regions.
- Logarithmic transformation made within the formula makes it difficult to infer the influence that each individual parameter has on the likelihood of planning success. - There is additional complexity and computational requirements to generate a predicted GWR model for a large geographic area. Regression models must be interpolated between the sample points.
- The main packages for GWR (*spgwr* [@R-spgwr] and GWmodel [@R-GWmodel]) are unable to conduct spatial predictions for logistic regression models.

Due to these concerns and limitations, the model was built using the *global parsimonious model*. This model contained `r variables_pars %>% length` variables, two of which were non-spatial parameters, *Turbine Capacity* and *Year*. To include these within the prediction, fixed values were assumed, with a Turbine Size of 2MW (the average size of wind turbines in 2016 as shown in Section \@ref(TurbineSize)) and predictions made for the year 2017.

```{r BuildRasterStack,  eval = params$runcode}
# Load the proximity rasters generated earlier in the analysis
# A RasterStack is formed for the datasets. A RasterStack has the advantage of facilitating pixel based calculations on separate raster layers.
# Create a list of the parameters in the model which should be loaded

dir_rasters <- "../data/process/4_BaselineExtraction/Rasters" # Raster File Location
raster_names <- variables_pars[!variables_pars %in% c("Turbine.Capacity..MW.", "year", "Railway_Trans")] %>%
  paste0(. , ".tif")

# Load the rasters into a raster stack
raster_list <- LoadRaster(dir_rasters, raster_names, message = FALSE)
raster_stack <- raster::stack(raster_list)
rm(raster_list)

# Although there is a function designed for this purpose, it has issues working with the logistic regression model built for this analysis. A more manual approach was therefore used, converting the raster to an XY data table as follows
grid_values <- rasterToPoints(raster_stack) %>% as.data.frame() %>% .[complete.cases(.),]
grid_values$year <- var_study_year
grid_values$'Turbine.Capacity..MW.' <- var_turbine_size
grid_values$Predicted <- predict.glm(object = model_pars, grid_values, type = 'response') # Calculate predicted values

# Converted prediction into raster and reproject
raster_predicted <- grid_values[, c(1,2, ncol(grid_values))] # Summarise dataset into XYZ table
raster_predicted <- suppressWarnings(raster::rasterFromXYZ(raster_predicted))
crs(raster_predicted) <- crs(raster_stack)

# Realign extents and add Predicted raster to stack
raster_stack <- crop(raster_stack, raster_predicted)
raster_stack <- stack(raster_predicted, raster_stack)

# Save the raster stack as an R object
save(raster_stack, file = "../data/process/6_Statistics/raster_stack.RData")  
save(raster_predicted, file = "../data/process/6_Statistics/raster_predicted.RData")  

# Clean up memory
rm(grid_values)
```

```{r LoadRasterStack}
# Loads the raster stack if the full analysis isn't completed
load("../data/process/6_Statistics/raster_stack.RData")  
load("../data/process/6_Statistics/raster_predicted.RData")  
```


```{r LoadStudyBoundaries}
# Load finalised raster if full analysis not run
StudyExtent <- shapefile("../data/input/geospatial/boundaries/BritainCoastlineNoShetland.shp")
Midland <- shapefile("../data/input/boundaries/MidlandsRegion.shp")
Solent <- shapefile("../data/input/boundaries/SolentRegion.shp")

# Format the data for plotting in ggplot
ggplot_extent_UK <- suppressMessages(StudyExtent %>% spTransform(crs_wgs84) %>% fortify())
ggplot_extent_Midlands <- suppressMessages(spTransform(Midland, crs_wgs84) %>% fortify())
ggplot_extent_Solent <- suppressMessages(spTransform(Solent, crs_wgs84) %>% fortify())
```

In order to gain a local understanding of the suitability of the model, case study areas were selected to conduct detailed comparative analysis. Two regions were selected, the *Midlands* (centre `r MidlandLoc`) and *Solent* (centre `r SolentLoc`), as shown in Figures \@ref(fig:CaseStudyExtentMap)b and \@ref(fig:CaseStudyExtentMap)c respectively. These regions broadly represent the LEP regions, although slight adjustments were made to the Midlands region to ensure a similar sized area as the Solent Area to aid comparison. 

```{r CaseStudyExtentMap, fig.cap="Analysis Extent and regions selected for case studies", message=FALSE, warning=FALSE, cache = params$cache}
map_UK <- 
  Basemaps$UKalt + 
  geom_polygon(data = ggplot_extent_Solent, aes(long, lat, group = group), fill = "steelblue3", col = "black", alpha = 0.5, size = 0.2) +
  geom_polygon(data = ggplot_extent_Midlands, aes(long, lat),  fill = "seagreen1", col = "black", alpha = 0.5, size = 0.2) +
  map_extent_GB + 
  labs(caption = "(a) Overview Map") +
  theme_map_square() +
  theme(plot.caption = element_text(hjust=0.5, size=rel(1)))

map_midlands <- 
  Basemaps$MidlandsLab + 
  geom_polygon(data = ggplot_extent_Midlands, aes(long, lat, group = group), fill = NA, col = "grey30", alpha = 0.1, size = 0.5) +
  coord_map(ylim = c(52.1, 53), xlim = c(-2.1 ,  -0.62)) +
  labs(caption = "(b) Midlands Region") +
  theme_map_square() +
  theme(plot.caption = element_text(hjust=0.5, size=rel(1)))

map_solent <- Basemaps$SolentLab +  
  geom_polygon(data = ggplot_extent_Solent, aes(long, lat, group = group), fill = NA, col = "grey30", alpha = 0.1, size = 0.5) +
  coord_map(ylim = c(50.4, 51.43), xlim = c(-2 ,  -0.4)) +
  labs(caption = "(c) Solent Region") +
  theme_map_square() +
  theme(plot.caption = element_text(hjust=0.5, size=rel(1)))

# Plot the results
grid.arrange(map_UK, map_midlands, map_solent, ncol = 3)
# Add turbine sites to map?
#rm(map_UK, map_solent, map_midlands)
```

The two study areas were selected for several reasons:

- The regions are similar size (`r round(area(Solent)/1000000,0)`km and `r round(area(Midland)/1000000, 0)`km^2^ respectively) with a mix of rural and urban areas. Solent region covers an estimated population of 1.1 million, while the Midlands region has a population of 1.3 million [@ONS2017]. 
- Both regions have extensive rural areas which have been considered for wind resources in previous developments; there are, however, large differences in the number of wind turbines actually deployed, with no wind turbines in the Solent region while 30 projects have been constructed within the Midlands region [@DECC2016].
- The author has local knowledge of the Solent region and knowledge of projects which have been proposed in the area. Such an insight is important to be able to explore the local context of the modelling results.

To further explore the regional variation, a selection of planning decisions for wind energy developments within the Solent area were reviewed, with a total of 8 projects considered^[Details of these projects are provided within a supplementary table in Appendix X.] This provided a way of assessing the predictive results against a sample of previous sites within the region.

Predictions of site acceptance were made across the study region at a resolution of 500 metres, which was selected as it is the recommended minimum spacing between commercial scale onshore wind turbines [@Smith2016]. The predictions were generated using the *stats* package in R [@R-base].

### Results

```{r RasterPredictedResultsSummary}
res_pred <- NULL
res_pred$mean <- cellStats(raster_predicted, mean) %>% round(2) %>% multiply_by(100)
res_pred$min <- cellStats(raster_predicted, min) %>% round(2) %>% multiply_by(100)
res_pred$max <- cellStats(raster_predicted, max) %>% round(2) %>% multiply_by(100)
res_pred$above50 <- cellStats(raster_predicted > 0.5, mean) %>% round(3) %>% multiply_by(100)
res_pred$below25 <- cellStats(raster_predicted < 0.25, mean) %>% round(3) %>% multiply_by(100)
```

The resulting site acceptability maps are shown in Figure \@ref(fig:PlotPredictedRasters), providing a prediction of the likelihood of a project receiving planning acceptance. The global average acceptance rates for the overall model was `r res_pred$mean`% with the range of individual sites from `r res_pred$min` to `r res_pred$max`. Only `r res_pred$above50`% of sites scored greater than 50% site acceptability, with `r res_pred$below25`% of locations estimated of having less than 25% chance of acceptance.

(ref:capPlotPredictedRasters) Maps highlighting the likelihood of wind energy projects receiving planning permission, as derived from the parsimonious logistic regression model. A turbine size of 2MW was assumed, and predictions were made for the year 2017.


```{r PlotPredictedRasters, fig.cap="(ref:capPlotPredictedRasters)", message=FALSE, warning=FALSE, out.width="100%", cache = params$cache}

# Reproject the raster for plotting
raster_predicted_wgs <- projectRaster(raster_predicted, crs = crs_wgs84, method = "ngb")

cutpoints <- function(x) cut(x, breaks=seq(0,1,0.1), labels=c("0-10","10-20","20-30","30-40", "40-50", "50-60", "60-70", "70-80", "80-90", "90-100"))

# Map Settings
colScale <- scale_fill_manual( values = c("#a50026", "#d73027", "#f46d43", "#fdae61", "#fee08b", "#d9ef8b", "#a6d96a", "#66bd63", "#1a9850", "#006837"))

# National Raster
map_predict_national <- 
  ggplot_rastermap(raster_predicted_wgs, StudyExtent, crs = crs_wgs84, title = "(a) National Map", colourScale = colScale, clipRaster = FALSE, cuts = cutpoints) +
  map_extent_GB_rast +
  guides(fill=guide_legend(title="Prediction")) +
  geom_polygon(data = ggplot_extent_Midlands, aes(long, lat, group = group), fill = NA, col = "tomato1", size = 0.3) +
  geom_polygon(data = ggplot_extent_Solent, aes(long, lat, group = group), fill = NA, col = "tomato1", size = 0.3)

# Custom function "SensitivityRasterMap" used to make the regional maps
map_predict_midlands <- 
  ggplot_rastermap(raster_predicted_wgs, Midland, crs_wgs84, title = "(b) Midlands Region", colScale, cuts = cutpoints) +
  map_extent_midlands_rast +
  theme(legend.position = "none")


map_predict_solent <-
  ggplot_rastermap(raster_predicted_wgs, Solent, crs_wgs84, title = "(c) Solent Region",colourScale = colScale, clipRaster = TRUE, cuts = cutpoints) +
  map_extent_solent_rast +
  theme(legend.position = "none")

# Create Output
grid.arrange(map_predict_national, map_predict_midlands, map_predict_solent,
             layout_matrix = rbind(c(1,2), c(1,3)) , widths= c(2,1), heights = c(1,1))


# Clean up after plot
rm(map_predict_national, map_predict_midlands, map_predict_solent, ggplot_predicted_map)
```

A detailed breakdown of the regional case studies against the national average is shown in Table \@ref(tab:PredictionSummaryTable). No site within the Solent area is rated higher than 25% chance of acceptance. The Midlands region appears more suitable, although again no region exceeds a 50% estimated chance of planning success.

\vspace{10mm}

```{r PredictionSummaryTable}
summary_prediction <- lapply(X = c(StudyExtent, Midland, Solent), function(x) AreaStats(x, raster_predicted)) %>% do.call(rbind, .) %>%
    as.data.frame() %>%  set_rownames(c("GB", "Midlands", "Solent"))
colnames(summary_prediction)[7] <- "Area"

knitr::kable(summary_prediction,
             caption = "Summary statistics comparing region suitability of onshore wind within the study regions", booktabs = TRUE, format = "latex") %>%
  kable_styling(full_width = T, latex_options = "hold_position")
```


```{r MakingRasterHistograms, eval = FALSE}
RastertoFreq <- function(raster, areaName){
  freq_table <- freq(round(raster * 100, 0), useNA = "no") %>%
    as.data.frame()
  freq_table$Percentage <- round(freq_table$count * 100/ sum(freq_table$count), 4)
  freq_table$Region <- areaName
  return(freq_table)
}
  
freq_table <- RastertoFreq(raster_predicted_wgs, "Great Britain") %>%
  rbind(RastertoFreq(raster_stack_solent$Predicted, "Solent")) %>%
  rbind(RastertoFreq(raster_stack_midlands$Predicted, "Midland"))

ggplot(freq_table, aes(value, Percentage, fill = Region), colour = "black") + geom_histogram(stat = 'identity') + coord_flip() + facet_wrap(~Region) +
  theme(legend.position = "none")

```

Detailed results are presented for the two cases study areas in Figures \@ref(fig:PredictionCaseStudy), highlighting the value of each variable within the two study regions. These can be combined with the knowledge of odds ratios as shown within the statistical modelling to infer the determining factors of suitability within each region, as will be discussed further in Section \@ref(GeneralisationDiscussion).

```{r CalculatePredictionCaseStudy, message=FALSE, warning=FALSE}
# Make Local raster stack
raster_stack_solent <- ClipRaster(raster_stack, Solent)
raster_stack_midlands <- ClipRaster(raster_stack, Midland)

# Uses custom function "ggplot_facet_raster_stack" to display
plot_layers_solent <- 
  ggplot_facet_raster_stack(raster_stack_solent, ggplot_extent_Solent, title = "Solent Region", ncols=5, Downsample = 2) + 
  coord_fixed(ylim = c(50.4, 51.43), xlim = c(-2 ,  -0.4),  ratio=1.3)

plot_layers_solent <- set_panel_size(plot_layers_solent)

plot_layers_midlands <-  
  ggplot_facet_raster_stack(raster_stack_midlands, ggplot_extent_Midlands, title = "Midland Region", ncols=5, Downsample = 2) +
  coord_fixed(ylim = c(52.1, 53), xlim = c(-2.1 ,  -0.62),  ratio=1.3)

plot_layers_midlands <- set_panel_size(plot_layers_midlands)
```

\afterpage{\clearpage}
```{r PredictionCaseStudy, fig.cap="Predicted outcome and variable mapping for the cast study regions.", fig.subcap= c('Parameter layers for Midlands region', 'Parameter layers for the Solent region'), fig.width=20, fig.height=10, out.width = "95%", fig.ncol = 1, fig.pos="p", cache = params$cache}
grid.arrange(plot_layers_midlands)
grid.arrange(plot_layers_solent)
rm(plot_layers_solent, plot_layers_midlands)
```

### Discussion {#GeneralisationDiscussion}

The national prediction raster shown in Figure \@ref(fig:PlotPredictedRasters) a) highlights that there are large regional variations within wind energy site acceptability. For example, large regions in Scotland appear suitable for development, while many other regions with the South of England appear "off limits" to development, particularly the regions along the South Coast of the UK. 

For the overall model, there is a low average predicted acceptance rate of `r res_pred$mean`%, which is below the rate of acceptance of wind energy in the UK, which was 40% in 2016 [@DECC2016]. It would be expected that the model would return a lower average, as sites which are selected by developers will pass through several preselection criteria prior to planning permission [@Smith2016]. Therefore, sites which are generally opposed before planning will often be abandoned before being taken to planning permission. 

The analysis results highlight that there is no "one-size-fits-all" approach, and that there are large regional variations in the development of wind energy projects beyond the availability of the resource. In comparison, the regional renewable energy studies conducted within the UK in 2010 broadly followed a consistent methodology to assess the resource potential, with small differences in the development rules in particular regard to environmental and landscape designations [@Stoddart2012]. Chapter \@ref(GISMCDA) therefore explores the implication of these findings on the resource potential of onshore wind.

Surprisingly, the model suggests that the South West of England has a low likelihood of acceptance, despite having high levels of wind energy within the area. The UK's wind energy development largely started in the region, so it had generally been considered supportive of wind energy. Studies have however noted that the region has a higher level of wind energy support in the region [@Eltham2008].  

#### Case Studies {.unnumbered}

Despite the Solent and Midlands being similar in area and population, the model indicates large differences in the acceptability of wind energy. On average, the results suggest area that projects are `r percent(summary_prediction$mean[2] - summary_prediction$mean[3])` more likely to receive planning within the Midlands than in the Solent. To explore the cause of this difference, the layer values shown in Figure \@ref(fig:PredictionCaseStudy) can be combined with the parameter influence expressed within the Odds Ratios in the Section \@ref(ParameterRefinement) to determine influential parameters. The following regional difference can be observed to explain the difference:

- **Nature and Landscape Designations**: 24% of the Solent region contains National Parks or Areas of Outstanding Natural Beauty.^[New Forest, South Downs National Park, and Areas of Outstanding Natural Beauty on the Isle of Wight] The proximity to such sites lessens the chance of project approval.
- **Demographics**: The demographic composition within the Solent region is generally higher qualified, with a higher mean age. Both these factors were suggested to reduce the chance of planning approval.
- **Labour council share**: the statistical model indicated that increased level of labour counsellors resulted in higher acceptance of wind energy. The Midlands region has a high coverage, while there is a low representation within the Solent region which is largely Conservative led.
- **Proximity to existing wind energy projects**: For regions in the Solent area, there are no existing turbines in the study area, with the nearest project over 50km from the region, while a number of projects have been rejected planning permission in the region. In comparison, there are 30 active projects within the Midlands region, with greater proximity to sites permission.

Within the review of planning applications of projects proposed in the Solent region, it was consistently found that the AONB and landscape designations were cited as reasons for their rejection [@TBC2013; @IOW2010]. There is difficulty in observing any direct relationship between political and demographic variables, as these in themselves cannot be used to refuse the planning decision. However, as discussed previously, these variables are argued to have an indirect impact on the planning process through more effective campaigning against projects.

The statistical model indicated that *Labour council seat share* positively influences the acceptance of wind energy projects, and it can be seen within the Midlands region that there are areas with 100% representation. However, it can generally be observed that Labour voters are concentrated in urban areas [@Dunleavy1979], and this view was supported within the preliminary data analysis in Chapter \@ref(DataChecks), which highlighted the spatial correlation between Labour voters and Large Urban areas. While wind energy may be more acceptable to in these population groups, it is unlikely that suitable sites for development will be found due to the land requirements which are not necessarily available in urban areas. This therefore highlights the importance of integrating the results of this sensitivity modelling with traditional onshore wind GIS approaches to identify sites which are both socially acceptable and feasibly developable, a topic which is explored further within Chapter \@ref(GISMCDA).

#### Application of Findings

While the model provides useful regional insights, it was shown within Chapter \@ref(TurbineStatisticalAnalyis) that the overall model fit was relatively low, with a Psuedo R^2^ value of `r 0.19`. However, with the estimated cost of planning applications for commercial scale projects exceeding \pounds 50000 [@RF2016], there is large value in even marginal improvements in the site selection. The findings from this model can help inform regional level strategy and provide an insight to developers of where projects may be more suitable for future development.

As highlighted within the literature, there have been recent changes in legislation regarding the planning of wind energy. These grant greater controls to local communities to oppose the development of wind turbines [@Smith2016]. Such changes in planning likely have an effect on the underlying influences of wind energy acceptance rates, and therefore care is required forecasting the results of a model based on historic data into future predictions. However, such approaches still offer a useful insight for high-level regional forecasting.

```{r}
# Clean up session
rm(raster_stack, raster_predicted, raster_predicted_wgs, raster_stack_midlands, raster_stack_solent)
```

## Conclusion of Acceptance Rates Analysis

This chapter investigated the influence of geospatial, environmental, demographic and political attributes on the probability of wind farm planning approval in Great Britain between 1990 and 2016. Four regression modelling techniques were developed and iteratively refined to improve the overall suitability of the model.

The findings of this work reveal that local demographic and political parameters appear to influence the planning outcomes of projects, and that many of the geospatial parameters typically integrated into wind energy models appear insignificant in determining site approval. To the authors' knowledge, such quantitative findings have not previously been demonstrated for onshore wind, and support the conclusions existing  qualitative studies. 

It appears that certain demographics are less accepting of onshore wind in Great Britain. Given that UK planning policy has now devolved power locally and allowing local communities to have the final say on projects [@Smith2016], there may be a clear limitation to development in certain regions in the country.

In addition, the results raise concerns of the predictive ability of existing geospatial modelling in locating wind energy sites. These findings provide evidence to support existing literature that GIS tools in themselves are of limited applicability [@Toke2005; @Malczewski2004], and supports the conclusion that greater emphasis needs to be given to the non-physical elements of a project (e.g. Community engagement with the scheme from an early stage) [@Toke2008; @Wolsink2000; @Warren2010].

A number of statistically significant parameters were identified within the research. The results appear to confirm existing literature by indicating that the distance to urban areas positively affects acceptance rates: however, there is a large amount of uncertainty as can be seen in the error bars. It is also interesting to note that socio-demographic characteristics have been indicated as being significant: the model suggests that regions with higher mean age and qualifications are less accepting of wind energy.

While the models have indicated a range of significant relationships, there are concerns surrounding the overall model fit, with relatively poor R^2^ values and predictive accuracy. This clearly reflects the concerns raised within literature that geospatial tools in isolation fail to accurately model the local challenges within renewable energy development [@Strantzali2016; @Malczewski2006]. These findings suggest that geospatial modelling techniques should not be considered entirely in isolation, and that they are best combined with the development process of wind energy. For example, it was shown in the literature review that projects proposed by local communities are generally more accepted than those proposed by large energy companies [@Toke2008].

In isolation, the results of this chapter are limited in their direct applicability for locating suitable sites for development, as the chance of a project receiving planning permission is just one aspect of a complex selection criteria for wind energy. The results of this chapter are therefore integrated with MCDA techniques in the following chapter to identify locations for development and assess regional capacity estimates.

```{block, type = "Summary", echo = TRUE}
## Chapter Summary {.unnumbered}

- Preliminary data checks identified several issues with the dataset, with missing data being replaced, and censoring conducted to limit extreme values.
- Key parameters which influence the acceptance rates of wind energy include 1) Turbine Capacity; 2) Year of Construction; 3) Demographic variables (mean age and level of qualifications) and 4) Political variables.
- Analysis suggests that proximity to existing wind energy developments increases the chance of a project being successful, countering the tradition "*proximity hypthesis*" proposed by literature.
- The model accuracy at predicting wind energy acceptance rates is relatively low with an overall model fit of 0.2, highlighting the importance of non-geospatial parameters within modelling.

```